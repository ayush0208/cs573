{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import trees as q2\n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calcu_Accu(label,pred,threshold):\n",
    "    count=0\n",
    "    for i in range(len(label)):\n",
    "        if pred[i]<threshold and label[i]==0:\n",
    "            count+=1\n",
    "        elif pred[i]>threshold and label[i]==1:\n",
    "            count+=1\n",
    "    return count/float(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tanh(x):\n",
    "#     return np.tanh(x)\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "# def tanh_deriv(x):\n",
    "#     return 1.0 - np.tanh(x)*np.tanh(x)\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "# class NN(object):\n",
    "#     def __init__(self, layers, activation):\n",
    "#         \"\"\"\n",
    "#         :param layers: list specify the number of neurons in each layer\n",
    "#         :param activation: only supports sigmoid and tanh\n",
    "#         \"\"\"\n",
    "#         self.layers=layers\n",
    "#         if activation == 'sigmoid':\n",
    "#             self.activation = sigmoid\n",
    "#             self.activation_deriv = sigmoid_deriv\n",
    "#             self.threshold=0.5\n",
    "#         elif activation == 'tanh':\n",
    "#             self.activation = tanh\n",
    "#             self.activation_deriv = tanh_deriv\n",
    "#             self.threshold = 0\n",
    "#         self.weights = []\n",
    "#         for i in range(1, len(layers)-1):\n",
    "#             self.weights.append(np.random.random((layers[i - 1] + 1, layers[i] + 1))-0.5)\n",
    "#         self.weights.append(np.random.random((layers[len(layers) - 2] + 1, layers[len(layers)-1])) - 0.5)\n",
    "#     def fit(self, X, y, learning_rate, epochs):\n",
    "#         X = np.atleast_2d(X)#Number of examples*features\n",
    "#         temp = np.ones([X.shape[0], X.shape[1] + 1])#add 1 vector for bias\n",
    "#         temp[:, 0:-1] = X\n",
    "#         X = temp\n",
    "#         y = np.array(y)\n",
    "#         update_weight=[0]*len(self.weights)\n",
    "#         for k in range(epochs):\n",
    "#             #combine = list(zip(X,y))\n",
    "#             #random.shuffle(combine)\n",
    "#             #X[:], y[:] = zip(*combine)\n",
    "#             for i in range(len(X)):\n",
    "#                 example=X[i]\n",
    "#                 pred=[example]\n",
    "\n",
    "#                 for l in range(len(self.weights)):\n",
    "#                     #if l==0 and i==0:\n",
    "#                     #    print(self.weights[l])\n",
    "#                     #print(pred[l].shape)\n",
    "#                     pred.append(self.activation(np.dot(pred[l],self.weights[l])))\n",
    "\n",
    "#                 error=y[i]-pred[-1]\n",
    "#                 deltas=[error*self.activation_deriv(pred[-1])]\n",
    "#                 for l in range(len(pred) - 2, 0, -1):\n",
    "#                     deltas.append(deltas[-1].dot(self.weights[l].T) * self.activation_deriv(pred[l]))\n",
    "#                 deltas.reverse()\n",
    "#                 for l in range(len(self.weights)):\n",
    "#                     layer = np.atleast_2d(pred[l])\n",
    "#                     delta = np.atleast_2d(deltas[l])\n",
    "#                     update_weight[l] += learning_rate * layer.T.dot(delta)\n",
    "#             for l in range(len(self.weights)):\n",
    "#                 self.weights[l]+=update_weight[l]\n",
    "\n",
    "#             #Calculate the accuracy\n",
    "#             #pred = self.pred_array(X[:,:-1])\n",
    "#             #train_accu = Calcu_Accu(y, pred, self.threshold)\n",
    "#             #print('%d epoch: Training Accuracy NN: %.2f' % (k,train_accu))\n",
    "#     def predict(self, x):\n",
    "#         x = np.array(x)\n",
    "#         temp = np.ones(x.shape[0] + 1)\n",
    "#         temp[0:-1] = x\n",
    "#         a = temp\n",
    "#         for l in range(0, len(self.weights)):\n",
    "#             a = self.activation(np.dot(a, self.weights[l]))\n",
    "#         return a\n",
    "#     def pred_array(self,array):\n",
    "#         pred=[]\n",
    "#         for i in range(len(array)):\n",
    "#             result=self.predict(array[i])\n",
    "#             pred.append(result)\n",
    "#         return pred\n",
    "def predict(x, weights):\n",
    "    x = np.array(x)\n",
    "    temp = np.ones(x.shape[0] + 1)\n",
    "    temp[0:-1] = x\n",
    "    a = temp\n",
    "    for l in range(0, len(weights)):\n",
    "        a = sigmoid(np.dot(a, weights[l]))\n",
    "    return a\n",
    "\n",
    "def pred_array(array, weights):\n",
    "    pred=[]\n",
    "    for i in range(len(array)):\n",
    "        result=predict(array[i], weights)\n",
    "        pred.append(result)\n",
    "    return pred\n",
    "\n",
    "def fit(X, y, weights, learning_rate, epochs):\n",
    "    X = np.atleast_2d(X)#Number of examples*features\n",
    "    temp = np.ones([X.shape[0], X.shape[1] + 1])#add 1 vector for bias\n",
    "    temp[:, 0:-1] = X\n",
    "    X = temp\n",
    "    y = np.array(y)\n",
    "    update_weight=[0]*len(weights)\n",
    "    for k in range(epochs):\n",
    "        for i in range(len(X)):\n",
    "            example=X[i]\n",
    "            pred=[example]\n",
    "\n",
    "            for l in range(len(weights)):\n",
    "                pred.append(sigmoid(np.dot(pred[l],weights[l])))\n",
    "\n",
    "            error=y[i]-pred[-1]\n",
    "            deltas=[error*sigmoid_deriv(pred[-1])]\n",
    "            for l in range(len(pred) - 2, 0, -1):\n",
    "                deltas.append(deltas[-1].dot(weights[l].T) * sigmoid_deriv(pred[l]))\n",
    "            deltas.reverse()\n",
    "            for l in range(len(weights)):\n",
    "                layer = np.atleast_2d(pred[l])\n",
    "                delta = np.atleast_2d(deltas[l])\n",
    "                update_weight[l] += learning_rate * layer.T.dot(delta)\n",
    "        for l in range(len(weights)):\n",
    "            weights[l]+=update_weight[l]\n",
    "    return weights\n",
    "\n",
    "def neural_net(layers, train_set):\n",
    "    weights = []\n",
    "    for i in range(1, len(layers)-1):\n",
    "        weights.append(np.random.random((layers[i - 1] + 1, layers[i] + 1))-0.5)\n",
    "    weights.append(np.random.random((layers[len(layers) - 2] + 1, layers[len(layers)-1])) - 0.5)\n",
    "\n",
    "    weights = fit(np.array(train_set.iloc[:,:-1]), np.array(train_set.iloc[:,-1]), weights, 0.001,50)\n",
    "    return weights\n",
    "\n",
    "def get_model_accuracy(train_set, test_set, weights, threshold):\n",
    "    train_pred = pred_array(np.array(train_set.iloc[:,:-1]), weights)\n",
    "    train_accu=Calcu_Accu(np.array(train_set.iloc[:,-1]),train_pred,threshold)\n",
    "    print('Training Accuracy NN: %.2f' % train_accu)\n",
    "\n",
    "    test_pred = pred_array(np.array(test_set.iloc[:,:-1]), weights)\n",
    "    test_accu=Calcu_Accu(np.array(test_set.iloc[:,-1]),test_pred,threshold)\n",
    "    print('Testing Accuracy NN: %.2f' % test_accu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy NN: 0.74\n",
      "Testing Accuracy NN: 0.72\n"
     ]
    }
   ],
   "source": [
    "train_reader = pd.read_csv('trainingSet.csv')\n",
    "test_reader = pd.read_csv('testSet.csv')\n",
    "activation = 'sigmoid'\n",
    "# func = 'sigmoid'\n",
    "threshold = 0.5\n",
    "features=len(train_reader.columns)-1\n",
    "# Network=NN([features,50,20,1],func)\n",
    "weights = neural_net([features,50,20,1], train_reader)\n",
    "# Network.fit(np.array(train_reader.iloc[:,:-1]),np.array(train_reader.iloc[:,-1]),0.001,50)\n",
    "# pred=Network.pred_array(np.array(train_reader.iloc[:,:-1]))\n",
    "\n",
    "# train_accu=Calcu_Accu(np.array(train_reader.iloc[:,-1]),pred,threshold)\n",
    "# pred=Network.pred_array(np.array(test_reader.iloc[:,:-1]))\n",
    "# test_accu=Calcu_Accu(np.array(test_reader.iloc[:,-1]),pred,threshold)\n",
    "# print('Training Accuracy NN: %.2f' % train_accu)\n",
    "# print('Testing Accuracy NN: %.2f' % test_accu)\n",
    "get_model_accuracy(train_reader, test_reader, weights, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python preprocess-assg4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy NN: 0.72\n",
      "Testing Accuracy NN: 0.71\n"
     ]
    }
   ],
   "source": [
    "!python neuralNetwork.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fraction: 0.05\n",
      "Current fold: 0\n",
      "DT acc: 0.6173076923076923\n",
      "In BT\n",
      "Bagging acc 0.6692307692307692\n",
      "In RF\n",
      "Random Forest acc 0.6480769230769231\n",
      "Current fold: 1\n",
      "DT acc: 0.6903846153846154\n",
      "In BT\n",
      "Bagging acc 0.7403846153846154\n",
      "In RF\n",
      "Random Forest acc 0.7326923076923076\n",
      "Current fold: 2\n",
      "DT acc: 0.7038461538461539\n",
      "In BT\n",
      "Bagging acc 0.7096153846153846\n",
      "In RF\n",
      "Random Forest acc 0.6846153846153846\n",
      "Current fold: 3\n",
      "DT acc: 0.7192307692307692\n",
      "In BT\n",
      "Bagging acc 0.7173076923076923\n",
      "In RF\n",
      "Random Forest acc 0.7211538461538461\n",
      "Current fold: 4\n",
      "DT acc: 0.7269230769230769\n",
      "In BT\n",
      "Bagging acc 0.7230769230769231\n",
      "In RF\n",
      "Random Forest acc 0.7384615384615385\n",
      "Current fold: 5\n",
      "DT acc: 0.7019230769230769\n",
      "In BT\n",
      "Bagging acc 0.7211538461538461\n",
      "In RF\n",
      "Random Forest acc 0.7269230769230769\n",
      "Current fold: 6\n",
      "DT acc: 0.6576923076923077\n",
      "In BT\n",
      "Bagging acc 0.698076923076923\n",
      "In RF\n",
      "Random Forest acc 0.7153846153846154\n",
      "Current fold: 7\n",
      "DT acc: 0.7269230769230769\n",
      "In BT\n",
      "Bagging acc 0.7038461538461539\n",
      "In RF\n",
      "Random Forest acc 0.7192307692307692\n",
      "Current fold: 8\n",
      "DT acc: 0.75\n",
      "In BT\n",
      "Bagging acc 0.7480769230769231\n",
      "In RF\n",
      "Random Forest acc 0.7211538461538461\n",
      "Current fold: 9\n",
      "DT acc: 0.6480769230769231\n",
      "In BT\n",
      "Bagging acc 0.6576923076923077\n",
      "In RF\n",
      "Random Forest acc 0.698076923076923\n",
      "DT:0.6942307692307692, Bagging:0.7088461538461539, Random Forest:0.710576923076923\n",
      "Current fraction: 0.075\n",
      "Current fold: 0\n",
      "DT acc: 0.6403846153846153\n",
      "In BT\n",
      "Bagging acc 0.7019230769230769\n",
      "In RF\n",
      "Random Forest acc 0.6788461538461539\n",
      "Current fold: 1\n",
      "DT acc: 0.7134615384615385\n",
      "In BT\n",
      "Bagging acc 0.7461538461538462\n",
      "In RF\n",
      "Random Forest acc 0.7576923076923077\n",
      "Current fold: 2\n",
      "DT acc: 0.6692307692307692\n",
      "In BT\n",
      "Bagging acc 0.7230769230769231\n",
      "In RF\n",
      "Random Forest acc 0.6865384615384615\n",
      "Current fold: 3\n",
      "DT acc: 0.6923076923076923\n",
      "In BT\n",
      "Bagging acc 0.7269230769230769\n",
      "In RF\n",
      "Random Forest acc 0.7307692307692307\n",
      "Current fold: 4\n",
      "DT acc: 0.6865384615384615\n",
      "In BT\n",
      "Bagging acc 0.7153846153846154\n",
      "In RF\n",
      "Random Forest acc 0.7384615384615385\n",
      "Current fold: 5\n",
      "DT acc: 0.7307692307692307\n",
      "In BT\n",
      "Bagging acc 0.7230769230769231\n",
      "In RF\n",
      "Random Forest acc 0.7326923076923076\n",
      "Current fold: 6\n",
      "DT acc: 0.676923076923077\n",
      "In BT\n",
      "Bagging acc 0.7096153846153846\n",
      "In RF\n",
      "Random Forest acc 0.6730769230769231\n",
      "Current fold: 7\n",
      "DT acc: 0.6923076923076923\n",
      "In BT\n",
      "Bagging acc 0.698076923076923\n",
      "In RF\n",
      "Random Forest acc 0.7173076923076923\n",
      "Current fold: 8\n",
      "DT acc: 0.7153846153846154\n",
      "In BT\n",
      "Bagging acc 0.7442307692307693\n",
      "In RF\n",
      "Random Forest acc 0.7096153846153846\n",
      "Current fold: 9\n",
      "DT acc: 0.7076923076923077\n",
      "In BT\n",
      "Bagging acc 0.7019230769230769\n",
      "In RF\n",
      "Random Forest acc 0.6807692307692308\n",
      "DT:0.6925000000000001, Bagging:0.7190384615384614, Random Forest:0.710576923076923\n",
      "Current fraction: 0.1\n",
      "Current fold: 0\n",
      "DT acc: 0.6730769230769231\n",
      "In BT\n",
      "Bagging acc 0.676923076923077\n",
      "In RF\n",
      "Random Forest acc 0.675\n",
      "Current fold: 1\n",
      "DT acc: 0.7076923076923077\n",
      "In BT\n",
      "Bagging acc 0.7384615384615385\n",
      "In RF\n",
      "Random Forest acc 0.7076923076923077\n",
      "Current fold: 2\n",
      "DT acc: 0.7\n",
      "In BT\n",
      "Bagging acc 0.7230769230769231\n",
      "In RF\n",
      "Random Forest acc 0.6942307692307692\n",
      "Current fold: 3\n",
      "DT acc: 0.7326923076923076\n",
      "In BT\n",
      "Bagging acc 0.7307692307692307\n",
      "In RF\n",
      "Random Forest acc 0.7346153846153847\n",
      "Current fold: 4\n",
      "DT acc: 0.7538461538461538\n",
      "In BT\n",
      "Bagging acc 0.7519230769230769\n",
      "In RF\n",
      "Random Forest acc 0.7461538461538462\n",
      "Current fold: 5\n",
      "DT acc: 0.7173076923076923\n",
      "In BT\n",
      "Bagging acc 0.725\n",
      "In RF\n",
      "Random Forest acc 0.725\n",
      "Current fold: 6\n",
      "DT acc: 0.7134615384615385\n",
      "In BT\n",
      "Bagging acc 0.7288461538461538\n",
      "In RF\n",
      "Random Forest acc 0.7153846153846154\n",
      "Current fold: 7\n",
      "DT acc: 0.7\n",
      "In BT\n",
      "Bagging acc 0.7173076923076923\n",
      "In RF\n",
      "Random Forest acc 0.7384615384615385\n",
      "Current fold: 8\n",
      "DT acc: 0.7403846153846154\n",
      "In BT\n",
      "Bagging acc 0.7403846153846154\n",
      "In RF\n",
      "Random Forest acc 0.7134615384615385\n",
      "Current fold: 9\n",
      "DT acc: 0.7057692307692308\n",
      "In BT\n",
      "Bagging acc 0.7115384615384616\n",
      "In RF\n",
      "Random Forest acc 0.7096153846153846\n",
      "DT:0.7144230769230769, Bagging:0.724423076923077, Random Forest:0.7159615384615384\n",
      "Current fraction: 0.15\n",
      "Current fold: 0\n",
      "DT acc: 0.698076923076923\n",
      "In BT\n",
      "Bagging acc 0.6961538461538461\n",
      "In RF\n",
      "Random Forest acc 0.7038461538461539\n",
      "Current fold: 1\n",
      "DT acc: 0.698076923076923\n",
      "In BT\n",
      "Bagging acc 0.75\n",
      "In RF\n",
      "Random Forest acc 0.7596153846153846\n",
      "Current fold: 2\n",
      "DT acc: 0.7096153846153846\n",
      "In BT\n",
      "Bagging acc 0.7134615384615385\n",
      "In RF\n",
      "Random Forest acc 0.6942307692307692\n",
      "Current fold: 3\n",
      "DT acc: 0.7211538461538461\n",
      "In BT\n",
      "Bagging acc 0.7326923076923076\n",
      "In RF\n",
      "Random Forest acc 0.7326923076923076\n",
      "Current fold: 4\n",
      "DT acc: 0.7211538461538461\n",
      "In BT\n",
      "Bagging acc 0.7442307692307693\n",
      "In RF\n",
      "Random Forest acc 0.7403846153846154\n",
      "Current fold: 5\n",
      "DT acc: 0.7019230769230769\n",
      "In BT\n",
      "Bagging acc 0.7230769230769231\n",
      "In RF\n",
      "Random Forest acc 0.75\n",
      "Current fold: 6\n",
      "DT acc: 0.7134615384615385\n",
      "In BT\n",
      "Bagging acc 0.725\n",
      "In RF\n",
      "Random Forest acc 0.7038461538461539\n",
      "Current fold: 7\n",
      "DT acc: 0.7\n",
      "In BT\n",
      "Bagging acc 0.7211538461538461\n",
      "In RF\n",
      "Random Forest acc 0.7538461538461538\n",
      "Current fold: 8\n",
      "DT acc: 0.7423076923076923\n",
      "In BT\n",
      "Bagging acc 0.75\n",
      "In RF\n",
      "Random Forest acc 0.7423076923076923\n",
      "Current fold: 9\n",
      "DT acc: 0.7057692307692308\n",
      "In BT\n",
      "Bagging acc 0.7076923076923077\n",
      "In RF\n",
      "Random Forest acc 0.7057692307692308\n",
      "DT:0.7111538461538461, Bagging:0.7263461538461539, Random Forest:0.7286538461538462\n",
      "Current fraction: 0.2\n",
      "Current fold: 0\n",
      "DT acc: 0.6923076923076923\n",
      "In BT\n",
      "Bagging acc 0.7076923076923077\n",
      "In RF\n",
      "Random Forest acc 0.6961538461538461\n",
      "Current fold: 1\n",
      "DT acc: 0.7153846153846154\n",
      "In BT\n",
      "Bagging acc 0.7596153846153846\n",
      "In RF\n",
      "Random Forest acc 0.7826923076923077\n",
      "Current fold: 2\n",
      "DT acc: 0.7096153846153846\n",
      "In BT\n",
      "Bagging acc 0.7\n",
      "In RF\n",
      "Random Forest acc 0.7442307692307693\n",
      "Current fold: 3\n",
      "DT acc: 0.7365384615384616\n",
      "In BT\n",
      "Bagging acc 0.7461538461538462\n",
      "In RF\n",
      "Random Forest acc 0.7576923076923077\n",
      "Current fold: 4\n",
      "DT acc: 0.7211538461538461\n",
      "In BT\n",
      "Bagging acc 0.7576923076923077\n",
      "In RF\n",
      "Random Forest acc 0.75\n",
      "Current fold: 5\n",
      "DT acc: 0.6961538461538461\n",
      "In BT\n",
      "Bagging acc 0.725\n",
      "In RF\n",
      "Random Forest acc 0.7326923076923076\n",
      "Current fold: 6\n",
      "DT acc: 0.7173076923076923\n",
      "In BT\n",
      "Bagging acc 0.725\n",
      "In RF\n",
      "Random Forest acc 0.7019230769230769\n",
      "Current fold: 7\n",
      "DT acc: 0.6942307692307692\n",
      "In BT\n",
      "Bagging acc 0.7096153846153846\n",
      "In RF\n",
      "Random Forest acc 0.7384615384615385\n",
      "Current fold: 8\n",
      "DT acc: 0.7519230769230769\n",
      "In BT\n",
      "Bagging acc 0.7596153846153846\n",
      "In RF\n",
      "Random Forest acc 0.7403846153846154\n",
      "Current fold: 9\n",
      "DT acc: 0.7192307692307692\n",
      "In BT\n",
      "Bagging acc 0.7096153846153846\n",
      "In RF\n",
      "Random Forest acc 0.7\n",
      "DT:0.7153846153846153, Bagging:0.7299999999999999, Random Forest:0.7344230769230771\n",
      "Figure(640x480)\n",
      "Value of p: 0.5781413680356962\n",
      "Accepting null hypothesis\n"
     ]
    }
   ],
   "source": [
    "!python cv_frac.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy DT: 0.77\n",
      "Testing Accuracy DT: 0.72\n"
     ]
    }
   ],
   "source": [
    "!python trees.py trainingSet.csv testSet.csv 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In BT\n",
      "Training Accuracy BT: 0.79\n",
      "Testing Accuracy BT: 0.75\n"
     ]
    }
   ],
   "source": [
    "!python trees.py trainingSet.csv testSet.csv 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In RF\n",
      "Training Accuracy RF: 0.76\n",
      "Testing Accuracy RF: 0.73\n"
     ]
    }
   ],
   "source": [
    "!python trees.py trainingSet.csv testSet.csv 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current depth: 3\n",
      "Current fold: 0\n",
      "Current fold: 1\n",
      "Current fold: 2\n",
      "Current fold: 3\n",
      "Current fold: 4\n",
      "Current fold: 5\n",
      "Current fold: 6\n",
      "Current fold: 7\n",
      "Current fold: 8\n",
      "Current fold: 9\n",
      "Current depth: 5\n",
      "Current fold: 0\n",
      "Current fold: 1\n",
      "Current fold: 2\n",
      "Current fold: 3\n",
      "Current fold: 4\n",
      "Current fold: 5\n",
      "Current fold: 6\n",
      "Current fold: 7\n",
      "Current fold: 8\n",
      "Current fold: 9\n",
      "Current depth: 7\n",
      "Current fold: 0\n",
      "Current fold: 1\n",
      "Current fold: 2\n",
      "Current fold: 3\n",
      "Current fold: 4\n",
      "Current fold: 5\n",
      "Current fold: 6\n",
      "Current fold: 7\n",
      "Current fold: 8\n",
      "Current fold: 9\n",
      "Current depth: 9\n",
      "Current fold: 0\n",
      "Current fold: 1\n",
      "Current fold: 2\n",
      "Current fold: 3\n",
      "Current fold: 4\n",
      "Current fold: 5\n",
      "Current fold: 6\n",
      "Current fold: 7\n",
      "Current fold: 8\n",
      "Current fold: 9\n",
      "Figure(640x480)\n",
      "Value of p: 0.06426597760804982\n",
      "Accepting null hypothesis\n",
      "Current depth: 3\n",
      "Current fold: 0\n",
      "Current fold: 1\n",
      "Current fold: 2\n",
      "Current fold: 3\n",
      "Current fold: 4\n",
      "Current fold: 5\n",
      "Current fold: 6\n",
      "Current fold: 7\n",
      "Current fold: 8\n",
      "Current fold: 9\n",
      "Current depth: 5\n",
      "Current fold: 0\n",
      "Current fold: 1\n",
      "Current fold: 2\n",
      "Current fold: 3\n",
      "Current fold: 4\n",
      "Current fold: 5\n",
      "Current fold: 6\n",
      "Current fold: 7\n",
      "Current fold: 8\n",
      "Current fold: 9\n",
      "Current depth: 7\n",
      "Current fold: 0\n",
      "Current fold: 1\n",
      "Current fold: 2\n",
      "Current fold: 3\n",
      "Current fold: 4\n",
      "Current fold: 5\n",
      "Current fold: 6\n",
      "Current fold: 7\n",
      "Current fold: 8\n",
      "Current fold: 9\n",
      "Current depth: 9\n",
      "Current fold: 0\n",
      "Current fold: 1\n",
      "Current fold: 2\n",
      "Current fold: 3\n",
      "Current fold: 4\n",
      "Current fold: 5\n",
      "Current fold: 6\n",
      "Current fold: 7\n",
      "Current fold: 8\n",
      "Current fold: 9\n",
      "Figure(640x480)\n",
      "Value of p: 0.2180349476133319\n",
      "Accepting null hypothesis\n"
     ]
    }
   ],
   "source": [
    "!python cv_depth.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python cv_numtrees.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
