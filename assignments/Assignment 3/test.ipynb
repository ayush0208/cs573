{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6500, 53)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_rows = 6500\n",
    "df = pd.read_csv('./dating-full.csv', nrows=num_of_rows)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_single_quote(s):\n",
    "    if s.startswith(\"'\") or s.endswith(\"'\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def strip_quotes(df, strip_quotes_list):\n",
    "    count = 0\n",
    "    for index in df.index:\n",
    "        for col in strip_quotes_list:\n",
    "            if contains_single_quote(df.loc[index, col]):\n",
    "                count+=1\n",
    "                df.loc[index, col] = df.loc[index, col].replace('\\'','')\n",
    "    return count\n",
    "\n",
    "# for 1-a-ii problem\n",
    "def to_lower(df):\n",
    "    val = df['field'].values\n",
    "    count=0\n",
    "    for value in val:\n",
    "        if not(value.islower()):\n",
    "            count+=1 \n",
    "    df['field'] = df['field'].str.lower()\n",
    "    return df, count\n",
    "\n",
    "def normalize(df, preference_scores_of_participant, preference_scores_of_partner):\n",
    "    for ind in df.index:\n",
    "        total=0\n",
    "        for col in preference_scores_of_participant:\n",
    "            total+= df[col][ind]\n",
    "        for col in preference_scores_of_participant:\n",
    "            df.loc[ind, col] = df[col][ind]/total\n",
    "        total=0\n",
    "        for col in preference_scores_of_partner:\n",
    "            total+= df[col][ind]\n",
    "        for col in preference_scores_of_partner:\n",
    "            df.loc[ind, col] = df[col][ind]/total\n",
    "    for col in preference_scores_of_participant:\n",
    "        print(\"Mean of \"+ col + \": \" ,round(df[[col]].mean()[0], 2))\n",
    "    for col in preference_scores_of_partner:\n",
    "        print(\"Mean of \"+ col + \": \" , round(df[[col]].mean()[0], 2))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quotes removed from 7947 cells\n",
      "Standardized 5463 cells to lower case\n",
      "Mean of attractive_important:  0.22\n",
      "Mean of sincere_important:  0.18\n",
      "Mean of intelligence_important:  0.2\n",
      "Mean of funny_important:  0.17\n",
      "Mean of ambition_important:  0.11\n",
      "Mean of shared_interests_important:  0.12\n",
      "Mean of pref_o_attractive:  0.22\n",
      "Mean of pref_o_sincere:  0.17\n",
      "Mean of pref_o_intelligence:  0.2\n",
      "Mean of pref_o_funny:  0.17\n",
      "Mean of pref_o_ambitious:  0.11\n",
      "Mean of pref_o_shared_interests:  0.12\n"
     ]
    }
   ],
   "source": [
    "strip_quotes_list = ['race','race_o','field']\n",
    "count = strip_quotes(df, strip_quotes_list)\n",
    "print(\"Quotes removed from \" + str(count) + \" cells\")\n",
    "\n",
    "# for 1-b\n",
    "df, count = to_lower(df)\n",
    "print(\"Standardized \" + str(count) + \" cells to lower case\")\n",
    "\n",
    "# for 1-d\n",
    "preference_scores_of_participant = ['attractive_important', 'sincere_important', 'intelligence_important', 'funny_important', 'ambition_important', 'shared_interests_important']\n",
    "preference_scores_of_partner = ['pref_o_attractive', 'pref_o_sincere', 'pref_o_intelligence', 'pref_o_funny', 'pref_o_ambitious', 'pref_o_shared_interests']\n",
    "\n",
    "df = normalize(df, preference_scores_of_participant, preference_scores_of_partner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped vector for female in column gender:  [1]\n",
      "Mapped vector for economics in column field:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "one_hot_encoding_list = ['gender','race','race_o','field']\n",
    "for i in range (0,df.shape[0]): #removing spaces from data\n",
    "\tdf[one_hot_encoding_list[0]].values[i] = df[one_hot_encoding_list[0]].values[i].strip()\n",
    "unq_count = len(df[one_hot_encoding_list[0]].unique())\n",
    "unq_val =  df[one_hot_encoding_list[0]].unique()\n",
    "unq_val_sorted = sorted(unq_val)\n",
    "#print unq_gender_count, unq_gender_val, unq_gender_val_sorted #ok\n",
    "\n",
    "# for i in range (0,df.shape[0]): #removing spaces from data\n",
    "# \tdf.race.values[i] = df.race.values[i].strip()\n",
    "# unq_race_count = len(df.race.unique())\n",
    "# unq_race_val =  df.race.unique()\n",
    "# unq_race_val_sorted = sorted(unq_race_val)\n",
    "# #print unq_race_count, unq_race_val, unq_race_val_sorted  #ok\n",
    "\n",
    "# for i in range (0,df.shape[0]): #removing spaces from data\n",
    "# \tdf.race_o.values[i] = df.race_o.values[i].strip()\n",
    "# unq_race_o_count = len(df.race_o.unique())\n",
    "# unq_race_o_val =  df.race_o.unique()\n",
    "# unq_race_o_val_sorted = sorted(unq_race_o_val)\n",
    "#print unq_race_o_count, unq_race_o_val, unq_race_o_val_sorted #ok\n",
    "\n",
    "\n",
    "for i in range (0,df.shape[0]): #removing spaces from data\n",
    "\tdf.field.values[i] = df.field.values[i].strip()\n",
    "unq_field_count = len(df.field.unique())\n",
    "unq_field_val =  df.field.unique()\n",
    "unq_field_val_sorted = sorted(unq_field_val)\n",
    "\n",
    "S = pd.Series( {one_hot_encoding_list[0]: unq_val })\n",
    "#print(S)\n",
    "one_hot = pd.get_dummies(S[one_hot_encoding_list[0]])\n",
    "'''\n",
    "print(one_hot)\n",
    "one_hot.drop(columns = ['male'], inplace=True) #drop last column\n",
    "print(one_hot)\n",
    "one_hot.drop(one_hot.tail(1).index,inplace=True) # drop last row\n",
    "print(one_hot)\n",
    "'''\n",
    "female_arr = []\n",
    "for i in range (0, one_hot.shape[0] - 1):\n",
    "\tfemale_arr.append(one_hot['female'].values[i])\n",
    "print (\"Mapped vector for female in column gender: \", female_arr)\n",
    "\n",
    "\n",
    "\n",
    "# S = pd.Series( {'race': unq_race_val_sorted})\n",
    "# #print(S)\n",
    "# one_hot = pd.get_dummies(S['race'])\n",
    "# #print(one_hot['Black/African American'])\n",
    "# blck_aa_arr = []\n",
    "# for i in range (0, one_hot.shape[0] - 1):\n",
    "# \tblck_aa_arr.append(one_hot['Black/African American'].values[i])\n",
    "# print (\"Mapped vector for Black/African American in column race: \", blck_aa_arr)\n",
    "\n",
    "\n",
    "# S = pd.Series( {'race_o': unq_race_o_val_sorted})\n",
    "# #print(S)\n",
    "# one_hot = pd.get_dummies(S['race_o'])\n",
    "# #print(one_hot.Other)\n",
    "# other_arr = []\n",
    "# for i in range (0, one_hot.shape[0]-1):\n",
    "# \tother_arr.append(one_hot['Other'].values[i])\n",
    "# print (\"Mapped vector for Other in column race_o: \", other_arr)\n",
    "\n",
    "\n",
    "S = pd.Series( {'field': unq_field_val_sorted})\n",
    "#print(S)\n",
    "one_hot = pd.get_dummies(S['field'])\n",
    "#print(one_hot.economics)\n",
    "economics_arr = []\n",
    "for i in range (0, one_hot.shape[0]-1):\n",
    "\teconomics_arr.append(one_hot['economics'].values[i])\n",
    "print (\"Mapped vector for economics in column field: \", economics_arr)\n",
    "\n",
    "#print df.head(10)\n",
    "df = pd.get_dummies(data=df, columns=['gender', 'race','race_o','field'])\n",
    "#print df.shape # before dropping number of rows = 6500 , columns = 265\n",
    "df.drop(columns = ['gender_male'], axis=1, inplace=True)\n",
    "df.drop(columns = ['race_Other'], axis=1, inplace=True)\n",
    "df.drop(columns = ['race_o_Other'], axis=1, inplace=True)\n",
    "df.drop(columns = ['field_writing: literary nonfiction'], axis=1, inplace=True)\n",
    "#print df.head(10)\n",
    "#print df.shape  # number of rows = 6500 , columns = 261\n",
    "#print df.columns\n",
    "\n",
    "\n",
    "# one_hot_encoding_list = ['gender','race','race_o','field']\n",
    "# df = pd.get_dummies(df, columns = one_hot_encoding_list)\n",
    "\n",
    "# df['race']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'age_o', 'samerace', 'importance_same_race',\n",
       "       'importance_same_religion', 'pref_o_attractive', 'pref_o_sincere',\n",
       "       'pref_o_intelligence', 'pref_o_funny', 'pref_o_ambitious',\n",
       "       ...\n",
       "       'field_stats', 'field_tc [health ed]', 'field_teaching of english',\n",
       "       'field_tesol', 'field_theater', 'field_theatre management & producing',\n",
       "       'field_undergrad - gs', 'field_urban planning', 'field_working',\n",
       "       'field_writing: literary nonfiction'],\n",
       "      dtype='object', length=265)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = df.columns\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN 1300\n",
      "LEN 5200\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(df):\n",
    "    test_df = df.sample(random_state=25, frac=0.2)\n",
    "    print(\"LEN\", test_df.shape[0])\n",
    "    train_df = df.drop(test_df.index)\n",
    "    print(\"LEN\", train_df.shape[0])\n",
    "    return train_df, test_df\n",
    "\n",
    "def split_data(df, train_filename, test_filename):\n",
    "    # df = pd.read_csv(input_filename)\n",
    "    train_df, test_df = train_test_split(df)\n",
    "    train_df.to_csv(train_filename, index = False)\n",
    "    test_df.to_csv(test_filename, index = False)\n",
    "\n",
    "split_data(df, 'trainingSet.csv', 'testSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.special import expit\n",
    "# def sigmoid(z):\n",
    "#     return 1/(1+expit(-z))\n",
    "def sigmoid(z):\n",
    "\t    return(1 / (1 + np.exp(-z)))  # z = w.T.dot(x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func(features, target, weights, lamb):\n",
    "    scores = np.dot(features, weights)\n",
    "    cf = np.sum( target*scores - np.log(1 + np.exp(scores)) )  +  lamb/(2) * sum(weights**2)\n",
    "    return cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(features, target, lamb, num_steps, learning_rate, add_intercept = False):\n",
    "    if add_intercept:\n",
    "    \t#print \"features before\",features.shape\n",
    "        intercept = np.ones((features.shape[0], 1))\n",
    "        #print \"intercept\", intercept.shape\n",
    "        features = np.hstack((intercept, features))\n",
    "        #print \"features after\",features.shape\n",
    "    \n",
    "    weights = np.zeros(features.shape[1])\n",
    "    #print \"weights loop\", weights.shape\n",
    "   \n",
    "    \n",
    "    for step in range(0, num_steps):\n",
    "        w1 = np.array(weights)\n",
    "        scores = np.dot(features, weights)\n",
    "        predictions = sigmoid(scores)\n",
    "\n",
    "        # Update weights with gradient\n",
    "       \toutput_error_signal = target - predictions\n",
    "        gradient = np.dot(features.T, output_error_signal) +  (lamb * weights)\n",
    "        weights += learning_rate * gradient\n",
    "        w2 = np.array(weights)\n",
    "        #print \"w2\", w2, type(w2)\n",
    "        diff = np.subtract(w2, w1)\n",
    "        #print \"diff\", diff\n",
    "        l2 = norm(diff)\n",
    "        #print \"l2\", l2\n",
    "        if l2 < 0.000001:\n",
    "        \tbreak\n",
    "        \n",
    "    \t\n",
    "        # Print log-likelihood every so often\n",
    "        #if step % 10 == 0:\n",
    "            #print cost_func(features, target, weights)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5200, 1)\n",
      "(5200, 264)\n",
      "(5200, 265)\n",
      "(5200, 261)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garga\\AppData\\Local\\Temp/ipykernel_20896/1239250713.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  return(1 / (1 + np.exp(-z)))  # z = w.T.dot(x_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy LR: 0.66\n",
      "Hello\n",
      "Testing Accuracy LR: 0.66\n"
     ]
    }
   ],
   "source": [
    "modelIdx = '1'\n",
    "df1 = pd.read_csv( 'trainingSet.csv') #training set\n",
    "print(df1.shape)\n",
    "df2 = pd.read_csv('testSet.csv') #test set\n",
    "\n",
    "if modelIdx == '1': #LR\n",
    "\n",
    "\t# X_train = df1.values[0:, 0:260]\n",
    "\t# Y_train = df1.values[0:, 260:261]\n",
    "\tY_train = df1['decision'].values\n",
    "\tlist_col = list(df1.columns)\n",
    "\tlist_col.remove('decision')\n",
    "\tX_train = df1[list_col].values\n",
    "\t#print \"X_train\", type(X_train), X_train\n",
    "\t#print Y_train, len(Y_train)\n",
    "\t#y = (df1.iloc[:, 261:262]).values\n",
    "\t#print y, len(y)\n",
    "\t#y_train = y_train.ravel()\n",
    "\t# X_test = df2.values[0:, 0:260]\n",
    "\tY_test = df2['decision'].values\n",
    "\t# Y_test = df1['decision'].values\n",
    "\t# list_col = list(df2.columns)\n",
    "\t# list_col.remove('decision')\n",
    "\tX_test = df2[list_col].values\n",
    "\t#y_test = y_test.ravel()\n",
    "\n",
    "\n",
    "\n",
    "\t#Logistic Regression\n",
    "\n",
    "\ttrain_features = X_train\n",
    "\ttrain_labels = Y_train.reshape(Y_train.shape[0],)\n",
    "\ttest_label = Y_test.reshape(Y_test.shape[0],)\n",
    "\tlamb = 0.01 \n",
    "\n",
    "\t#learning part\n",
    "\t#step_size = learning rate\n",
    "\tweights = logistic_regression(train_features, train_labels, lamb=lamb, num_steps = 500, learning_rate = 0.01, add_intercept=True)\n",
    "\t#print \"weights\", weights.shape\n",
    "\t# prediction for training\n",
    "\tdata_with_intercept = np.hstack((np.ones((train_features.shape[0], 1)),train_features))\n",
    "\t#print \"data_with_intercept \",data_with_intercept.shape\n",
    "\tfinal_scores = np.dot(data_with_intercept, weights)\n",
    "\tpreds = np.round(sigmoid(final_scores))\n",
    "\ttrain_accuracy_LR = (preds == train_labels).sum().astype(float) / len(preds)\n",
    "\tprint ('Training Accuracy LR:', round(train_accuracy_LR,2))\n",
    "\n",
    "\t# prediction for testing \n",
    "\tdata_with_intercept2 = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "\tfinal_scores2 = np.dot(data_with_intercept2, weights)\n",
    "\tpreds2 = np.round(sigmoid(final_scores2))\n",
    "\t# print(\"Hello\")\n",
    "\ttest_accuracy_LR = (preds2 == test_label).sum().astype(float) / len(preds2) \n",
    "\tprint ('Testing Accuracy LR:',round(test_accuracy_LR,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(features, target, lamb, num_steps2, learning_rate, add_intercept = False):\n",
    "\tif add_intercept:\n",
    "\t\t#print \"features before\",features.shape\n",
    "\t\tintercept = np.ones((features.shape[0], 1))\n",
    "\t\t#print \"intercept\", intercept.shape\n",
    "\t\tfeatures = np.hstack((intercept, features))\n",
    "\t\t#print \"features after\",features.shape\n",
    "\t\t\n",
    "\tweights = np.zeros(features.shape[1])\n",
    "\tout = []\n",
    "\tN = target.shape[0]\n",
    "\t#print N\n",
    "\tfor step in range (0,num_steps2):\n",
    "\t\t#print \"step\", step\n",
    "\t\tfor i, val in enumerate(features):\n",
    "\t\t\tw1 = np.array(weights)\n",
    "\t\t\tval1 = np.dot(features[i],weights)\n",
    "\t\t\t#print val1\n",
    "\t\t\tif (target[i] * val1) < 1.0:\n",
    "\t\t\t\t#weights = weights + (learning_rate * ((target[i] * features[i]) - (2 *_lambda * weights)))\n",
    "\t\t\t\tweights = weights + (learning_rate * (((target[i] * features[i]) - (1 *lamb * weights))/N))\n",
    "\t\t\telse:\n",
    "\t\t\t\t#weights = weights + (learning_rate * (- (2 *_lambda * weights)))\n",
    "\t\t\t\tweights = weights + (learning_rate * (- ((1 *lamb * weights)/N)))\n",
    "\t\t\tw2 = np.array(weights)\n",
    "\t\t\t#print \"w2\", w2, type(w2)\n",
    "\t\t\tdiff = np.subtract(w2, w1)\n",
    "\t\t\t#print \"diff\", diff\n",
    "\t\t\tl2 = norm(diff)\n",
    "\t\t\t#print \"l2\", l2\n",
    "\t\t\tif l2 < 0.000001:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\treturn weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy for svm 0.43\n",
      "Testing Accuracy for svm:  0.44\n"
     ]
    }
   ],
   "source": [
    "modelIdx = '2'\n",
    "if modelIdx == '2': #Linear SVM\n",
    "\tY_train = df1['decision'].values\n",
    "\tlist_col = list(df1.columns)\n",
    "\tlist_col.remove('decision')\n",
    "\tX_train = df1[list_col].values\n",
    "\t#print \"Y_train\",Y_train\n",
    "\t#c = 0\n",
    "\tfor i,val in enumerate(Y_train):\n",
    "\t\tif Y_train[i] == 0:\n",
    "\t\t\tY_train[i] = -1\n",
    "\t\t\t#c+=1\n",
    "\tY_test = df2['decision'].values\n",
    "\tX_test = df2[list_col].values\n",
    "\t\n",
    "\t# X_train = df1.values[0:, 1:261]\n",
    "\t# Y_train = df1.values[0:, 261:262]\n",
    "\t#print \"Y_train\",Y_train\n",
    "\t#c = 0\n",
    "\tfor i,val in enumerate(Y_train):\n",
    "\t\tif Y_train[i] == 0:\n",
    "\t\t\tY_train[i] = -1\n",
    "\t\t\t#c+=1\n",
    "\t#print c\n",
    "\t#print Y_train\n",
    "\t#print \"X_train\", type(X_train), X_train\n",
    "\t# X_test = df2.values[0:, 1:261]\n",
    "\t# Y_test = df2.values[0:, 261:262]\n",
    "\t#print \"Y_test\", Y_test\n",
    "\tfor i,val in enumerate(Y_test):\n",
    "\t\tif Y_test[i] == 0:\n",
    "\t\t\tY_test[i] = -1\n",
    "\t#y_test = y_test.ravel()\n",
    "\n",
    "\ttrain_features = X_train\n",
    "\ttrain_labels = Y_train.reshape(Y_train.shape[0],)\n",
    "\ttest_label = Y_test.reshape(Y_test.shape[0],)\n",
    "\n",
    "\tlamb = 0.01 \n",
    "\n",
    "\t#learning part\n",
    "\t#step_size = learning rate\n",
    "\t#print \"X_train.shape\", X_train.shape\n",
    "\tweights = svm(train_features, train_labels, lamb=lamb, num_steps2 = 500, learning_rate = 0.5, add_intercept=False)\n",
    "\tfinal_scores = np.sign(np.dot(train_features,weights))\n",
    "\t#print final_scores\n",
    "\t'''\n",
    "\tc = 0\n",
    "\tfor i,val in enumerate(final_scores):\n",
    "\t\tif final_scores[i] == -1:\n",
    "\t\t\t#final_scores[i] = -1\n",
    "\t\t\tc+=1\n",
    "\tprint c\n",
    "\t'''\n",
    "\ttrain_accuracy_SVM = (final_scores == train_labels).sum().astype(float) / len(final_scores)\n",
    "\tprint ('Training Accuracy for svm',round(train_accuracy_SVM,2))\n",
    "\n",
    "\t# prediction for testing \n",
    "\tfinal_scores2 = np.sign(np.dot(X_test, weights))\n",
    "\ttest_accuracy_SVM = (final_scores2 == test_label).sum().astype(float) / len(final_scores2) \n",
    "\tprint('Testing Accuracy for svm: ', round(test_accuracy_SVM,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(features, weights, target, lamb):\n",
    "    scores = np.dot(features, weights)\n",
    "    cf = np.sum( target*scores - np.log(1 + expit(scores)) )  + ((lamb/(2)) * sum(weights**2))\n",
    "    return cf\n",
    "    # return -np.mean(y*(np.log(y_hat)) - (1-y)*np.log(1-y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y, y_hat, lamb, weights):\n",
    "    # m = X.shape[0]\n",
    "    dw = np.dot(X.T, (y_hat-y)) + lamb*weights\n",
    "    # db = (1/m)*np.sum(y_hat-y)\n",
    "    # return dw, db\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "def train(X, y, learning_rate, max_iter, lamb, add_intercept = True):\n",
    "    if add_intercept:\n",
    "    \t#print \"features before\",features.shape\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        #print \"intercept\", intercept.shape\n",
    "        X = np.hstack((intercept, X))\n",
    "        #print \"features after\",features.shape\n",
    "    \n",
    "    # X --> Input.\n",
    "    # y --> true/target value.\n",
    "    # bs --> Batch Size.\n",
    "    # epochs --> Number of iterations.\n",
    "    # lr --> Learning rate.\n",
    "        \n",
    "    # m-> number of training examples\n",
    "    # n-> number of features \n",
    "    # m, n = X.shape\n",
    "    \n",
    "    # Initializing weights and bias to zeros.\n",
    "    w = np.zeros(X.shape[1], dtype = int)\n",
    "    # b = 0\n",
    "    \n",
    "    # Reshaping y.\n",
    "    # y = y.reshape(m,1)\n",
    "    \n",
    "    # # Normalizing the inputs.\n",
    "    # x = normalize(X)\n",
    "    \n",
    "    # Empty list to store losses.\n",
    "    losses = []\n",
    "    \n",
    "    # Training loop.\n",
    "    for iter in range(max_iter):\n",
    "        # print(\"Iteration: \", iter)\n",
    "        w1 = np.array(w)\n",
    "        score = np.dot(X,w)\n",
    "        preds = sigmoid(score)\n",
    "        l = loss(X, w, y, lamb)\n",
    "        losses.append(l)\n",
    "        grad = gradient(X, y, preds, lamb, w)\n",
    "        w = w - learning_rate*grad\n",
    "        w2 = np.array(w)\n",
    "        if(norm(w2-w1) < 1e-6):\n",
    "            break\n",
    "    return w, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5200,)\n",
      "(5200, 264)\n",
      "(5200, 265)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('trainingSet.csv')\n",
    "y = train_df['decision'].values\n",
    "list_col = list(train_df.columns)\n",
    "list_col.remove('decision')\n",
    "\n",
    "X = train_df[list_col].values\n",
    "\n",
    "# train_df.shape\n",
    "# target_col = 'decision'\n",
    "# y=train_df[[target_col]]\n",
    "# train_df.drop(target_col, axis=1, inplace=True)\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "print(train_df.shape)\n",
    "# X = train_df.to_numpy()\n",
    "# X.shape\n",
    "# y = y.to_numpy()\n",
    "# y.shape\n",
    "# X = X.reshape(X.shape[0], )\n",
    "y = y.reshape(y.shape[0], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "w, losses = train(X, y, 0.01, 500, 0.01)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set:  1\n",
      "data_with_intercept  (5200, 265)\n",
      "Training Accuracy LR: 0.5651923076923077\n"
     ]
    }
   ],
   "source": [
    "data_with_intercept = np.hstack((np.ones((X.shape[0], 1)),X))\n",
    "print(\"train set: \", y.max())\n",
    "print (\"data_with_intercept \",data_with_intercept.shape)\n",
    "final_scores = np.dot(data_with_intercept, w)\n",
    "preds = np.round_(sigmoid(final_scores))\n",
    "train_accuracy_LR = (preds == y).sum().astype(float) / len(preds)\n",
    "print('Training Accuracy LR:', (train_accuracy_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1300,)\n",
      "(1300, 264)\n",
      "(1300, 265)\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('testSet.csv')\n",
    "X_test = test_df[list_col].values\n",
    "y_test = test_df['decision'].values\n",
    "# train_df.shape\n",
    "# target_col = 'decision'\n",
    "# y=train_df[[target_col]]\n",
    "# train_df.drop(target_col, axis=1, inplace=True)\n",
    "print(y_test.shape)\n",
    "print(X_test.shape)\n",
    "print(test_df.shape)\n",
    "# X = train_df.to_numpy()\n",
    "# X.shape\n",
    "# y = y.to_numpy()\n",
    "# y.shape\n",
    "# X = X.reshape(X.shape[0], )\n",
    "y_test = y_test.reshape(y_test.shape[0], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED2:  0\n",
      "Testing Accuracy LR: 0.5638461538461539\n"
     ]
    }
   ],
   "source": [
    "data_with_intercept2 = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "final_scores2 = np.dot(data_with_intercept2, w)\n",
    "preds2 = np.round_(sigmoid(final_scores2))\n",
    "print(\"PRED2: \", preds2.argmax())\n",
    "test_accuracy_LR = (preds2 == y_test).sum().astype(float) / len(preds2) \n",
    "print ('Testing Accuracy LR:', (test_accuracy_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'age_o',\n",
       " 'samerace',\n",
       " 'importance_same_race',\n",
       " 'importance_same_religion',\n",
       " 'pref_o_attractive',\n",
       " 'pref_o_sincere',\n",
       " 'pref_o_intelligence',\n",
       " 'pref_o_funny',\n",
       " 'pref_o_ambitious',\n",
       " 'pref_o_shared_interests',\n",
       " 'attractive_important',\n",
       " 'sincere_important',\n",
       " 'intelligence_important',\n",
       " 'funny_important',\n",
       " 'ambition_important',\n",
       " 'shared_interests_important',\n",
       " 'attractive',\n",
       " 'sincere',\n",
       " 'intelligence',\n",
       " 'funny',\n",
       " 'ambition',\n",
       " 'attractive_partner',\n",
       " 'sincere_partner',\n",
       " 'intelligence_parter',\n",
       " 'funny_partner',\n",
       " 'ambition_partner',\n",
       " 'shared_interests_partner',\n",
       " 'sports',\n",
       " 'tvsports',\n",
       " 'exercise',\n",
       " 'dining',\n",
       " 'museums',\n",
       " 'art',\n",
       " 'hiking',\n",
       " 'gaming',\n",
       " 'clubbing',\n",
       " 'reading',\n",
       " 'tv',\n",
       " 'theater',\n",
       " 'movies',\n",
       " 'concerts',\n",
       " 'music',\n",
       " 'shopping',\n",
       " 'yoga',\n",
       " 'interests_correlate',\n",
       " 'expected_happy_with_sd_people',\n",
       " 'like',\n",
       " 'decision',\n",
       " 'gender_female',\n",
       " 'gender_male',\n",
       " 'race_Asian/Pacific Islander/Asian-American',\n",
       " 'race_Black/African American',\n",
       " 'race_European/Caucasian-American',\n",
       " 'race_Latino/Hispanic American',\n",
       " 'race_Other',\n",
       " 'race_o_Asian/Pacific Islander/Asian-American',\n",
       " 'race_o_Black/African American',\n",
       " 'race_o_European/Caucasian-American',\n",
       " 'race_o_Latino/Hispanic American',\n",
       " 'race_o_Other',\n",
       " 'field_acting',\n",
       " 'field_african-american studies/history',\n",
       " 'field_american studies',\n",
       " 'field_american studies [masters]',\n",
       " 'field_anthropology',\n",
       " 'field_anthropology/education',\n",
       " 'field_applied maths/econs',\n",
       " 'field_applied physiology & nutrition',\n",
       " 'field_architecture',\n",
       " 'field_art education',\n",
       " 'field_art history',\n",
       " 'field_art history/medicine',\n",
       " 'field_arts administration',\n",
       " 'field_bilingual education',\n",
       " 'field_biochemistry',\n",
       " 'field_biochemistry & molecular biophysics',\n",
       " 'field_biochemistry/genetics',\n",
       " 'field_biology',\n",
       " 'field_biology phd',\n",
       " 'field_biomedical engineering',\n",
       " 'field_biomedical informatics',\n",
       " 'field_biomedicine',\n",
       " 'field_biotechnology',\n",
       " 'field_business',\n",
       " 'field_business & international affairs',\n",
       " 'field_business [finance & marketing]',\n",
       " 'field_business [mba]',\n",
       " 'field_business administration',\n",
       " 'field_business consulting',\n",
       " 'field_business school',\n",
       " 'field_business- mba',\n",
       " 'field_business/ finance/ real estate',\n",
       " 'field_business/law',\n",
       " 'field_business; marketing',\n",
       " 'field_business; media',\n",
       " 'field_cell biology',\n",
       " 'field_chemistry',\n",
       " 'field_classics',\n",
       " 'field_climate change',\n",
       " 'field_climate-earth and environ. science',\n",
       " 'field_clinical psychology',\n",
       " 'field_cognitive studies in education',\n",
       " 'field_communications',\n",
       " 'field_communications in education',\n",
       " 'field_comparative literature',\n",
       " 'field_computational biochemsistry',\n",
       " 'field_computer science',\n",
       " 'field_conservation biology',\n",
       " 'field_consulting',\n",
       " 'field_counseling psychology',\n",
       " 'field_creative writing',\n",
       " 'field_creative writing - nonfiction',\n",
       " 'field_creative writing [nonfiction]',\n",
       " 'field_curriculum and teaching/giftedness',\n",
       " 'field_early childhood education',\n",
       " 'field_earth and environmental science',\n",
       " 'field_ecology',\n",
       " 'field_economics',\n",
       " 'field_economics and political science',\n",
       " 'field_economics; english',\n",
       " 'field_economics; sociology',\n",
       " 'field_ed.d. in higher education policy at tc',\n",
       " 'field_education',\n",
       " 'field_education administration',\n",
       " 'field_education leadership - public school administration',\n",
       " 'field_education policy',\n",
       " 'field_education- literacy specialist',\n",
       " 'field_educational psychology',\n",
       " 'field_electrical engg.',\n",
       " 'field_electrical engineering',\n",
       " 'field_elementary education',\n",
       " 'field_elementary education - preservice',\n",
       " 'field_elementary/childhood education [ma]',\n",
       " 'field_engineering',\n",
       " 'field_english',\n",
       " 'field_english and comp lit',\n",
       " 'field_english education',\n",
       " 'field_environmental engineering',\n",
       " 'field_epidemiology',\n",
       " 'field_film',\n",
       " 'field_finanace',\n",
       " 'field_finance',\n",
       " 'field_finance&economics',\n",
       " 'field_finance/economics',\n",
       " 'field_financial engineering',\n",
       " 'field_financial math',\n",
       " 'field_french',\n",
       " 'field_fundraising management',\n",
       " 'field_genetics',\n",
       " 'field_genetics & development',\n",
       " 'field_german literature',\n",
       " 'field_gs postbacc premed',\n",
       " 'field_gsas',\n",
       " 'field_history',\n",
       " 'field_history [gsas - phd]',\n",
       " 'field_history of religion',\n",
       " 'field_human rights',\n",
       " 'field_human rights: middle east',\n",
       " 'field_instructional media and technology',\n",
       " 'field_instructional tech & media',\n",
       " 'field_intellectual property law',\n",
       " 'field_international affairs',\n",
       " 'field_international affairs - economic development',\n",
       " 'field_international affairs - economic policy',\n",
       " 'field_international affairs and public health',\n",
       " 'field_international affairs/finance',\n",
       " 'field_international affairs/international finance',\n",
       " 'field_international business',\n",
       " 'field_international development',\n",
       " 'field_international educational development',\n",
       " 'field_international finance and business',\n",
       " 'field_international politics',\n",
       " 'field_international relations',\n",
       " 'field_international security policy - sipa',\n",
       " 'field_intrernational affairs',\n",
       " 'field_japanese literature',\n",
       " 'field_journalism',\n",
       " 'field_law',\n",
       " 'field_law and english literature [j.d./ph.d.]',\n",
       " 'field_law and social work',\n",
       " 'field_law/business',\n",
       " 'field_ma biotechnology',\n",
       " 'field_ma in quantitative methods',\n",
       " 'field_ma science education',\n",
       " 'field_ma teaching social studies',\n",
       " 'field_marine geophysics',\n",
       " 'field_marketing',\n",
       " 'field_master in public administration',\n",
       " 'field_masters in public administration',\n",
       " 'field_masters of social work',\n",
       " 'field_masters of social work&education',\n",
       " 'field_math',\n",
       " 'field_math education',\n",
       " 'field_math of finance',\n",
       " 'field_mathematical finance',\n",
       " 'field_mathematics',\n",
       " 'field_mathematics; phd',\n",
       " 'field_mba',\n",
       " 'field_mba / master of international affairs [sipa]',\n",
       " 'field_mba finance',\n",
       " 'field_mechanical engineering',\n",
       " 'field_medical informatics',\n",
       " 'field_medicine',\n",
       " 'field_medicine and biochemistry',\n",
       " 'field_mfa  poetry',\n",
       " 'field_mfa -film',\n",
       " 'field_mfa acting program',\n",
       " 'field_mfa creative writing',\n",
       " 'field_mfa writing',\n",
       " 'field_microbiology',\n",
       " 'field_modern chinese literature',\n",
       " 'field_molecular biology',\n",
       " 'field_money',\n",
       " 'field_museum anthropology',\n",
       " 'field_music education',\n",
       " 'field_neurobiology',\n",
       " 'field_neuroscience',\n",
       " 'field_neurosciences/stem cells',\n",
       " 'field_nonfiction writing',\n",
       " 'field_nutrition',\n",
       " 'field_nutrition/genetics',\n",
       " 'field_nutritiron',\n",
       " 'field_operations research',\n",
       " 'field_operations research [seas]',\n",
       " 'field_organizational psychology',\n",
       " 'field_philosophy',\n",
       " 'field_philosophy [ph.d.]',\n",
       " 'field_philosophy and physics',\n",
       " 'field_physics',\n",
       " 'field_physics [astrophysics]',\n",
       " 'field_polish',\n",
       " 'field_political science',\n",
       " 'field_psychology',\n",
       " 'field_psychology and english',\n",
       " 'field_public administration',\n",
       " 'field_public health',\n",
       " 'field_public policy',\n",
       " 'field_qmss',\n",
       " 'field_religion',\n",
       " 'field_religion; gsas',\n",
       " 'field_school psychology',\n",
       " 'field_sipa - energy',\n",
       " 'field_sipa / mia',\n",
       " 'field_sipa-international affairs',\n",
       " 'field_soa -- writing',\n",
       " 'field_social studies education',\n",
       " 'field_social work',\n",
       " 'field_social work/sipa',\n",
       " 'field_sociology',\n",
       " 'field_sociology and education',\n",
       " 'field_sociomedical sciences- school of public health',\n",
       " 'field_speech language pathology',\n",
       " 'field_speech pathology',\n",
       " 'field_statistics',\n",
       " 'field_stats',\n",
       " 'field_tc [health ed]',\n",
       " 'field_teaching of english',\n",
       " 'field_tesol',\n",
       " 'field_theater',\n",
       " 'field_theatre management & producing',\n",
       " 'field_undergrad - gs',\n",
       " 'field_urban planning',\n",
       " 'field_working',\n",
       " 'field_writing: literary nonfiction']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if \"decision\" in train_df.columns:\n",
    "    print(\"found\")\n",
    "else:\n",
    "    print(\"no\")\n",
    "list(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    \n",
    "    # X --> Input.\n",
    "    \n",
    "    # Normalizing the inputs.\n",
    "    # x = normalize(X)\n",
    "    \n",
    "    # Calculating presictions/y_hat.\n",
    "    preds = sigmoid(np.dot(X, w) + b)\n",
    "    \n",
    "    # Empty List to store predictions.\n",
    "    pred_class = []\n",
    "    # if y_hat >= 0.5 --> round up to 1\n",
    "    # if y_hat < 0.5 --> round up to 1\n",
    "    pred_class = [1 if i > 0.5 else 0 for i in preds]\n",
    "    \n",
    "    return np.array(pred_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    accuracy = np.sum(y == y_hat) / len(y)\n",
    "    return accuracy\n",
    "# accuracy(X, y_hat=predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5200, 1)\n",
      "5200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5200, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('trainingSet.csv')\n",
    "train_df.shape\n",
    "target_col = 'decision'\n",
    "y=train_df[[target_col]]\n",
    "train_df.drop(target_col, axis=1, inplace=True)\n",
    "print(y.shape)\n",
    "print(train_df.shape[0])\n",
    "X = train_df.to_numpy()\n",
    "X.shape\n",
    "y = y.to_numpy()\n",
    "y.shape\n",
    "# list(y).reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_svm(data,w_new):\n",
    "    df = pd.DataFrame(data[data.columns.drop('decision')])\n",
    "    predict = np.dot(w_new,df.T)\n",
    "    predict = np.where(predict >= 0, 1, -1)\n",
    "    return predict\n",
    "\n",
    "def learnsvm(train, lamb, stepsize_svm):\n",
    "    svm_train = train\n",
    "    svm_train['intercept'] = 1\n",
    "    w_old = np.zeros(len(svm_train.columns)-1, dtype=int)\n",
    "    w_new = np.zeros(len(w_old), dtype=int)\n",
    "    iteration_count = 0\n",
    "    while (iteration_count<500):\n",
    "        if norm(np.subtract(w_new, w_old)) > (1e-6):     \n",
    "            w_old=w_new\n",
    "            df = pd.DataFrame(svm_train[svm_train.columns.drop('decision')])\n",
    "            np_df = df.as_matrix()\n",
    "            predict = np.dot(w_old,np_df.T)\n",
    "            predict = np.where(predict >= 0, 1, -1) \n",
    "            diff = (svm_train['decision']*predict)\n",
    "            B = svm_train[(svm_train['decision'] == 1) & (predict == 1)]\n",
    "            dec = B['decision']\n",
    "            B.drop('decision',axis =1,inplace=True)\n",
    "            gradient = np.dot(dec,B)\n",
    "            grad_intercept = gradient[len(gradient)-1]/len(svm_train)\n",
    "            grad_final = lamb*w_old[:len(w_old)-1] - gradient[:len(gradient)-1]/len(svm_train)\n",
    "            grad_intercept = np.array(grad_intercept)\n",
    "            grad_final = np.append(grad_final,grad_intercept)\n",
    "            w_new = w_old - stepsize_svm*grad_final\n",
    "            iteration_count +=1\n",
    "        else:\n",
    "            break\n",
    "    return w_new,svm_train\n",
    "\n",
    "def printaccuracy_svm(decision,prediction):\n",
    "    accuracy = np.where((decision & prediction), 1,0)\n",
    "    accuracy =  round(float((accuracy==0).sum())/((accuracy==0).sum()+(accuracy==1).sum()),2)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['decision'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20896/1948551359.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mw_news\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msvm_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearnsvm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction_svm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvm_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_news\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# print(\"Hello\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprintaccuracy_svm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'decision'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# print(\"Hello1\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20896/887959782.py\u001b[0m in \u001b[0;36mprediction_svm\u001b[1;34m(data, w_new)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprediction_svm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'decision'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_new\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6016\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6017\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6018\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6019\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6020\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['decision'] not found in axis\""
     ]
    }
   ],
   "source": [
    "w_news,svm_train = learnsvm(train_df, 0.01, 0.5) \n",
    "predict = prediction_svm(svm_train,w_news)\n",
    "# print(\"Hello\")\n",
    "train_accuracy = printaccuracy_svm(train_df['decision'],predict)\n",
    "# print(\"Hello1\")\n",
    "print (\"Training Accuracy SVM:\",train_accuracy)\n",
    "test_df['intercept'] = 1\n",
    "predict_test = prediction_svm(test_df,w_news)\n",
    "test_accuracy = printaccuracy_svm(test_df['decision'],predict_test)\n",
    "print (\"Test Accuracy SVM:\",test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_lr(data,w_new):\n",
    "    df = pd.DataFrame(data[data.columns.drop('decision')])\n",
    "    predict = 1 / (1 + np.exp(-(np.dot(w_new,df.T))))\n",
    "    predict = np.where(predict >= 0.5, 1, 0)\n",
    "    return predict\n",
    "def learnlr(train, max_itr_count, lamb, stepsize):\n",
    "    learn_train = train\n",
    "    learn_train['intercept'] = 1\n",
    "    w_old = np.zeros(len(learn_train.columns)-1, dtype=int)\n",
    "    w_new = np.zeros(len(w_old), dtype=int)\n",
    "    iteration_count = 0;   \n",
    "    while (iteration_count < max_itr_count):\n",
    "        if norm(np.subtract(w_new, w_old)) > (1e-6):\n",
    "            w_old = w_new\n",
    "            df = pd.DataFrame(learn_train[learn_train.columns.drop('decision')])\n",
    "            np_df = df.as_matrix()\n",
    "            predict = 1 / (1 + np.exp(-(np.dot(w_old,np_df.T))))\n",
    "            #predict = (predict>=0.5).astype(int)\n",
    "            predict = np.where(predict >= 0.5, 1, 0) \n",
    "            diff = (-learn_train['decision']+predict)\n",
    "            grad = np.dot(np_df.T,diff) + lamb*w_old\n",
    "            w_new = w_old - stepsize* grad\n",
    "            iteration_count +=1\n",
    "        else:\n",
    "            break   \n",
    "    return w_new,learn_train\n",
    "\n",
    "def printaccuracy_lr(decision,predict):\n",
    "    accuracy = abs(decision-predict)\n",
    "    accuracy =  round(float((accuracy==0).sum())/((accuracy==0).sum()+(accuracy==1).sum()),2)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['decision'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20896/771473243.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mw_new\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearn_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearnlr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprintaccuracy_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'decision'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Training Accuracy LR:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'intercept'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20896/1427770476.py\u001b[0m in \u001b[0;36mprediction_lr\u001b[1;34m(data, w_new)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprediction_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'decision'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_new\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6016\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6017\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6018\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6019\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6020\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['decision'] not found in axis\""
     ]
    }
   ],
   "source": [
    "w_new,learn_train = learnlr(train_df, 500, 0.01, 0.01) \n",
    "predict = prediction_lr(learn_train,w_new)\n",
    "train_accuracy = printaccuracy_lr(train_df['decision'],predict)\n",
    "print (\"Training Accuracy LR:\",train_accuracy)\n",
    "test_df['intercept'] = 1\n",
    "predict_test = prediction_lr(test_df,w_new)\n",
    "test_accuracy = printaccuracy_lr(test_df['decision'],predict_test)\n",
    "print (\"Test Accuracy LR:\",test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    [b, a, c]\n",
      "dtype: object\n",
      "   a  b  c\n",
      "0  0  1  0\n",
      "1  1  0  0\n",
      "2  0  0  1\n"
     ]
    }
   ],
   "source": [
    "S = pd.Series( {'A': ['b', 'a', 'c']})\n",
    "print(S)\n",
    "one_hot = pd.get_dummies(S['A'])\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(features, target, num_steps2, learning_rate, add_intercept = False):\n",
    "    if add_intercept:\n",
    "        #print \"features before\",features.shape\n",
    "        intercept = np.ones((features.shape[0], 1))\n",
    "        #print \"intercept\", intercept.shape\n",
    "        features = np.hstack((intercept, features))\n",
    "        #print \"features after\",features.shape\n",
    "\n",
    "    weights = np.zeros(features.shape[1])\n",
    "    out = []\n",
    "    N = target.shape[0]\n",
    "    #print N\n",
    "    for step in range (0,num_steps2):\n",
    "    #print \"step\", step\n",
    "        for i, val in enumerate(features):\n",
    "            w1 = np.array(weights)\n",
    "            val1 = np.dot(features[i],weights)\n",
    "            #print val1\n",
    "            if (target[i] * val1) < 1.0:\n",
    "                #weights = weights + (learning_rate * ((target[i] * features[i]) - (2 *_lambda * weights)))\n",
    "                weights = weights + (learning_rate * (((target[i] * features[i]) - (1 *_lambda * weights))/N))\n",
    "            else:\n",
    "                #weights = weights + (learning_rate * (- (2 *_lambda * weights)))\n",
    "                weights = weights + (learning_rate * (- ((1 *_lambda * weights)/N)))\n",
    "            w2 = np.array(weights)\n",
    "            #print \"w2\", w2, type(w2)\n",
    "            diff = np.subtract(w2, w1)\n",
    "            #print \"diff\", diff\n",
    "            l2 = norm(diff)\n",
    "            #print \"l2\", l2\n",
    "            if l2 < 0.000001:\n",
    "                break\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy for svm 0.43\n",
      "Testing Accuracy for svm:  0.44\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('trainingSet.csv')\n",
    "df2 = pd.read_csv('testSet.csv')\n",
    "Y_train = df1['decision'].values\n",
    "list_col = list(df1.columns)\n",
    "list_col.remove('decision')\n",
    "X_train = df1[list_col].values\n",
    "Y_test = df2['decision'].values\n",
    "X_test = df2[list_col].values\n",
    "\n",
    "\n",
    "\n",
    "#Logistic Regression\n",
    "\n",
    "# train_features = X_train\n",
    "# train_labels = Y_train.reshape(Y_train.shape[0],)\n",
    "# test_label = Y_test.reshape(Y_test.shape[0],)\n",
    "\n",
    "# X_train = df1.values[0:, 1:261]\n",
    "# Y_train = df1.values[0:, 261:262]\n",
    "#print \"Y_train\",Y_train\n",
    "#c = 0\n",
    "for i,val in enumerate(Y_train):\n",
    "    if Y_train[i] == 0:\n",
    "        Y_train[i] = -1\n",
    "        #c+=1\n",
    "#print c\n",
    "#print Y_train\n",
    "#print \"X_train\", type(X_train), X_train\n",
    "# X_test = df2.values[0:, 1:261]\n",
    "# Y_test = df2.values[0:, 261:262]\n",
    "#print \"Y_test\", Y_test\n",
    "for i,val in enumerate(Y_test):\n",
    "    if Y_test[i] == 0:\n",
    "        Y_test[i] = -1\n",
    "#y_test = y_test.ravel()\n",
    "\n",
    "# train_features = X_train\n",
    "# train_labels = Y_train.reshape(Y_train.shape[0],)\n",
    "# test_label = Y_test.reshape(Y_test.shape[0],)\n",
    "\n",
    "train_features = X_train\n",
    "train_labels = Y_train.reshape(Y_train.shape[0],)\n",
    "test_label = Y_test.reshape(Y_test.shape[0],)\n",
    "\n",
    "_lambda = 0.01 \n",
    "\n",
    "#learning part\n",
    "#step_size = learning rate\n",
    "#print \"X_train.shape\", X_train.shape\n",
    "weights = svm(train_features, train_labels, num_steps2 = 500, learning_rate = 0.5, add_intercept=False)\n",
    "final_scores = np.sign(np.dot(train_features,weights))\n",
    "#print final_scores\n",
    "'''\n",
    "c = 0\n",
    "for i,val in enumerate(final_scores):\n",
    "    if final_scores[i] == -1:\n",
    "        #final_scores[i] = -1\n",
    "        c+=1\n",
    "print c\n",
    "'''\n",
    "train_accuracy_SVM = (final_scores == train_labels).sum().astype(float) / len(final_scores)\n",
    "print ('Training Accuracy for svm',round(train_accuracy_SVM,2))\n",
    "\n",
    "# prediction for testing \n",
    "final_scores2 = np.sign(np.dot(X_test, weights))\n",
    "test_accuracy_SVM = (final_scores2 == test_label).sum().astype(float) / len(final_scores2) \n",
    "print ('Testing Accuracy for svm: ', round(test_accuracy_SVM,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4      -1\n",
       "       ..\n",
       "5195   -1\n",
       "5196   -1\n",
       "5197    1\n",
       "5198   -1\n",
       "5199   -1\n",
       "Name: decision, Length: 5200, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['decision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(X, y, weights, lamb):\n",
    "    # calculate hinge loss\n",
    "    N = X.shape[0]\n",
    "    distances = 1 - y * (np.dot(X, weights))\n",
    "    distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
    "    hinge_loss =  (np.sum(distances) / N)\n",
    "\n",
    "    # calculate cost\n",
    "    loss = lamb / 2 * np.dot(weights, weights) + hinge_loss\n",
    "    return loss\n",
    "\n",
    "def gradient_calculation(X, y, weights, lamb):\n",
    "    distance = 1 - y * (np.dot(X, weights))\n",
    "    dw = np.zeros(len(weights))\n",
    "    t = np.where(y <= 0, -1, 1)\n",
    "    for ind, d in enumerate(distance):\n",
    "        condition = t[ind] * (np.dot(X[ind], weights)) >= 1\n",
    "        if condition:\n",
    "            dw += lamb * weights\n",
    "        else:\n",
    "            dw += lamb * weights - np.dot(X[ind], t[ind])\n",
    "    return dw/len(y)\n",
    "\n",
    "def run_svm(X, y, lamb, max_iter, step_size):\n",
    "    bias = np.ones((X.shape[0], 1))\n",
    "    X = np.concatenate((bias, X), axis=1)\n",
    "    weights = np.zeros(X.shape[1])\n",
    "    for epoch in range(max_iter):\n",
    "        # idx = np.random.permutation(len(X))\n",
    "        # x_shuffled = X[idx]\n",
    "        # y_shuffled = y[idx]\n",
    "        prev_weights = weights\n",
    "        dw = gradient_calculation(X, y, weights, lamb)\n",
    "        diff =  step_size * dw\n",
    "        weights -= diff\n",
    "        if(norm(diff)< 0.000001):\n",
    "            print(\"Tolerance reached\")\n",
    "            break;\n",
    "        print('Epoch {} Loss: {}'.format(epoch, loss_function(X, y, weights, lamb)))\n",
    "    return weights\n",
    "        \n",
    "\n",
    "def predict(X, weights):\n",
    "    return np.sign(np.dot(X, weights))\n",
    "\n",
    "def accuracy(X, y, weights):\n",
    "    bias = np.ones((X.shape[0], 1))\n",
    "    X = np.concatenate((bias, X), axis=1)\n",
    "    preds = predict(X, weights)\n",
    "    t = np.where(y <= 0, -1, 1)\n",
    "    print(np.unique(preds, return_counts=True))\n",
    "    train_accuracy = (preds == t).sum()/t.size\n",
    "    print(\"Training Accuracy SVM:\" , round(train_accuracy, 2))\n",
    "\n",
    "def get_features_labels(df):\n",
    "    labels = df['decision']\n",
    "    train_features = df.drop('decision', 1)\n",
    "    return train_features.to_numpy(), labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./trainingSet.csv')\n",
    "X_train, y_train = get_features_labels(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 73.93535057441528\n",
      "Epoch 1 Loss: 0.9215100268135537\n",
      "Epoch 2 Loss: 147.58405042830537\n",
      "Epoch 3 Loss: 0.7229233331908861\n",
      "Epoch 4 Loss: 220.60933837885847\n",
      "Epoch 5 Loss: 0.6364418616150702\n",
      "Epoch 6 Loss: 293.01522491177735\n",
      "Epoch 7 Loss: 25.28417440644988\n",
      "Epoch 8 Loss: 1.2011844854110851\n",
      "Epoch 9 Loss: 99.38355576914381\n",
      "Epoch 10 Loss: 0.9655393399781433\n",
      "Epoch 11 Loss: 172.85571678510098\n",
      "Epoch 12 Loss: 0.8430493254385796\n",
      "Epoch 13 Loss: 245.70469228990882\n",
      "Epoch 14 Loss: 0.8453892263175766\n",
      "Epoch 15 Loss: 270.11428407739345\n",
      "Epoch 16 Loss: 5.4933469549659355\n",
      "Epoch 17 Loss: 0.9987625768368511\n",
      "Epoch 18 Loss: 214.4920993887582\n",
      "Epoch 19 Loss: 0.9552604068028063\n",
      "Epoch 20 Loss: 285.4782470371063\n",
      "Epoch 21 Loss: 18.27638754431692\n",
      "Epoch 22 Loss: 1.4878477035064674\n",
      "Epoch 23 Loss: 113.68333246299775\n",
      "Epoch 24 Loss: 1.319349421018702\n",
      "Epoch 25 Loss: 187.08216122187216\n",
      "Epoch 26 Loss: 1.2619468188854028\n",
      "Epoch 27 Loss: 259.3156268146182\n",
      "Epoch 28 Loss: 3.4071578364323223\n",
      "Epoch 29 Loss: 22.7354246485419\n",
      "Epoch 30 Loss: 1.733049698870563\n",
      "Epoch 31 Loss: 118.20120719133159\n",
      "Epoch 32 Loss: 1.591500075652878\n",
      "Epoch 33 Loss: 191.58407148831915\n",
      "Epoch 34 Loss: 1.560193823433485\n",
      "Epoch 35 Loss: 261.4244156001359\n",
      "Epoch 36 Loss: 5.14413958617454\n",
      "Epoch 37 Loss: 2.462638560470274\n",
      "Epoch 38 Loss: 108.77831626650894\n",
      "Epoch 39 Loss: 1.8116346445099678\n",
      "Epoch 40 Loss: 182.25524734929436\n",
      "Epoch 41 Loss: 1.7782052661284897\n",
      "Epoch 42 Loss: 252.7205033561176\n",
      "Epoch 43 Loss: 3.6058004176624623\n",
      "Epoch 44 Loss: 53.71236456038511\n",
      "Epoch 45 Loss: 2.2279830572132076\n",
      "Epoch 46 Loss: 129.36840511273107\n",
      "Epoch 47 Loss: 2.1317492252865025\n",
      "Epoch 48 Loss: 202.68962809080728\n",
      "Epoch 49 Loss: 2.1514300564045925\n",
      "Epoch 50 Loss: 249.17620924679318\n",
      "Epoch 51 Loss: 3.9312398221696485\n",
      "Epoch 52 Loss: 64.68308365446707\n",
      "Epoch 53 Loss: 2.5616804713859045\n",
      "Epoch 54 Loss: 139.47089460001976\n",
      "Epoch 55 Loss: 2.4949736123590722\n",
      "Epoch 56 Loss: 212.06252888278894\n",
      "Epoch 57 Loss: 2.592961358723891\n",
      "Epoch 58 Loss: 225.55301167780857\n",
      "Epoch 59 Loss: 2.8465219576302596\n",
      "Epoch 60 Loss: 183.38841189018152\n",
      "Epoch 61 Loss: 2.6971103918337627\n",
      "Epoch 62 Loss: 240.57450761361545\n",
      "Epoch 63 Loss: 3.9594891478135814\n",
      "Epoch 64 Loss: 102.8475268281405\n",
      "Epoch 65 Loss: 2.9967054730209233\n",
      "Epoch 66 Loss: 176.3234361644262\n",
      "Epoch 67 Loss: 3.002985936554028\n",
      "Epoch 68 Loss: 235.04750397251598\n",
      "Epoch 69 Loss: 3.9994416049318082\n",
      "Epoch 70 Loss: 119.29832740027311\n",
      "Epoch 71 Loss: 3.260711053137323\n",
      "Epoch 72 Loss: 191.87968044532542\n",
      "Epoch 73 Loss: 3.3225827773823466\n",
      "Epoch 74 Loss: 226.76663260363532\n",
      "Epoch 75 Loss: 3.930813227349246\n",
      "Epoch 76 Loss: 145.90016206272972\n",
      "Epoch 77 Loss: 3.500447278814326\n",
      "Epoch 78 Loss: 214.592613945203\n",
      "Epoch 79 Loss: 3.7997083760227617\n",
      "Epoch 80 Loss: 183.00812784737627\n",
      "Epoch 81 Loss: 3.674300565290441\n",
      "Epoch 82 Loss: 223.90874284132084\n",
      "Epoch 83 Loss: 4.300838025298427\n",
      "Epoch 84 Loss: 146.44888082060874\n",
      "Epoch 85 Loss: 3.8596236680273246\n",
      "Epoch 86 Loss: 213.13788244271305\n",
      "Epoch 87 Loss: 4.204358922613715\n",
      "Epoch 88 Loss: 175.24872725473438\n",
      "Epoch 89 Loss: 4.0370042478121295\n",
      "Epoch 90 Loss: 219.73479917710426\n",
      "Epoch 91 Loss: 4.615693706863177\n",
      "Epoch 92 Loss: 150.61948301309116\n",
      "Epoch 93 Loss: 4.22000326222044\n",
      "Epoch 94 Loss: 212.5480481019024\n",
      "Epoch 95 Loss: 4.630043323762129\n",
      "Epoch 96 Loss: 164.72181735882899\n",
      "Epoch 97 Loss: 4.4022120394701085\n",
      "Epoch 98 Loss: 214.64431918834885\n",
      "Epoch 99 Loss: 4.90958589360329\n",
      "Epoch 100 Loss: 154.86479611652237\n",
      "Epoch 101 Loss: 4.586550806716641\n",
      "Epoch 102 Loss: 211.4460713460983\n",
      "Epoch 103 Loss: 5.043816422072268\n",
      "Epoch 104 Loss: 158.64137218621008\n",
      "Epoch 105 Loss: 4.770459043625783\n",
      "Epoch 106 Loss: 210.09578488106652\n",
      "Epoch 107 Loss: 5.226564867116506\n",
      "Epoch 108 Loss: 158.35603264818363\n",
      "Epoch 109 Loss: 4.951786647111412\n",
      "Epoch 110 Loss: 208.3465338122884\n",
      "Epoch 111 Loss: 5.3967762337813685\n",
      "Epoch 112 Loss: 158.58206739371457\n",
      "Epoch 113 Loss: 5.132036193171995\n",
      "Epoch 114 Loss: 205.88998531675242\n",
      "Epoch 115 Loss: 5.546970405107183\n",
      "Epoch 116 Loss: 161.1129581985296\n",
      "Epoch 117 Loss: 5.313181120234996\n",
      "Epoch 118 Loss: 204.9403242795069\n",
      "Epoch 119 Loss: 5.7293315942062515\n",
      "Epoch 120 Loss: 159.50994861626853\n",
      "Epoch 121 Loss: 5.488373029450171\n",
      "Epoch 122 Loss: 203.24657484014233\n",
      "Epoch 123 Loss: 5.893481804925124\n",
      "Epoch 124 Loss: 159.98537345123611\n",
      "Epoch 125 Loss: 5.665431889369735\n",
      "Epoch 126 Loss: 201.75535755608152\n",
      "Epoch 127 Loss: 6.0594250810971895\n",
      "Epoch 128 Loss: 160.08962738608568\n",
      "Epoch 129 Loss: 5.840457616985301\n",
      "Epoch 130 Loss: 199.9832875496312\n",
      "Epoch 131 Loss: 6.216714790812165\n",
      "Epoch 132 Loss: 162.63863327141283\n",
      "Epoch 133 Loss: 6.017601795385001\n",
      "Epoch 134 Loss: 197.74712285375665\n",
      "Epoch 135 Loss: 6.360427110321722\n",
      "Epoch 136 Loss: 164.47452316671487\n",
      "Epoch 137 Loss: 6.1913485832930695\n",
      "Epoch 138 Loss: 195.77271418403973\n",
      "Epoch 139 Loss: 6.507345319029696\n",
      "Epoch 140 Loss: 165.7533711624216\n",
      "Epoch 141 Loss: 6.36083777649506\n",
      "Epoch 142 Loss: 193.64212840762167\n",
      "Epoch 143 Loss: 6.649140976304068\n",
      "Epoch 144 Loss: 166.83165914718205\n",
      "Epoch 145 Loss: 6.526857719099583\n",
      "Epoch 146 Loss: 190.71223847154286\n",
      "Epoch 147 Loss: 6.778405103127182\n",
      "Epoch 148 Loss: 169.9396078385635\n",
      "Epoch 149 Loss: 6.696332817645922\n",
      "Epoch 150 Loss: 187.4972261993319\n",
      "Epoch 151 Loss: 6.902824008613575\n",
      "Epoch 152 Loss: 173.09153094417172\n",
      "Epoch 153 Loss: 6.865350416716574\n",
      "Epoch 154 Loss: 184.28461433845345\n",
      "Epoch 155 Loss: 7.027490364379376\n",
      "Epoch 156 Loss: 175.56217070646312\n",
      "Epoch 157 Loss: 7.03325377200118\n",
      "Epoch 158 Loss: 180.94508542795623\n",
      "Epoch 159 Loss: 7.153689218356629\n",
      "Epoch 160 Loss: 175.85560026807232\n",
      "Epoch 161 Loss: 7.189328823357391\n",
      "Epoch 162 Loss: 179.24257185417997\n",
      "Epoch 163 Loss: 7.29375633275473\n",
      "Epoch 164 Loss: 175.81598082721194\n",
      "Epoch 165 Loss: 7.342063145833294\n",
      "Epoch 166 Loss: 177.60152235783534\n",
      "Epoch 167 Loss: 7.432761432533142\n",
      "Epoch 168 Loss: 175.4649955459063\n",
      "Epoch 169 Loss: 7.490986780012972\n",
      "Epoch 170 Loss: 176.40889052372268\n",
      "Epoch 171 Loss: 7.574226973325815\n",
      "Epoch 172 Loss: 174.72189084171782\n",
      "Epoch 173 Loss: 7.6355411694811135\n",
      "Epoch 174 Loss: 175.3298895654865\n",
      "Epoch 175 Loss: 7.715138177206428\n",
      "Epoch 176 Loss: 174.10436452278907\n",
      "Epoch 177 Loss: 7.779549801704963\n",
      "Epoch 178 Loss: 174.47524467578054\n",
      "Epoch 179 Loss: 7.856481110410354\n",
      "Epoch 180 Loss: 173.2589498052358\n",
      "Epoch 181 Loss: 7.920197523524367\n",
      "Epoch 182 Loss: 173.63895385707764\n",
      "Epoch 183 Loss: 7.996394434336029\n",
      "Epoch 184 Loss: 172.56469613555834\n",
      "Epoch 185 Loss: 8.060450130541435\n",
      "Epoch 186 Loss: 172.9522910619627\n",
      "Epoch 187 Loss: 8.136017979597009\n",
      "Epoch 188 Loss: 171.5270196860803\n",
      "Epoch 189 Loss: 8.196389382794388\n",
      "Epoch 190 Loss: 172.05898376979206\n",
      "Epoch 191 Loss: 8.272188707611644\n",
      "Epoch 192 Loss: 170.96676785326278\n",
      "Epoch 193 Loss: 8.334481198965346\n",
      "Epoch 194 Loss: 171.371552221011\n",
      "Epoch 195 Loss: 8.408527952484256\n",
      "Epoch 196 Loss: 169.95031934530135\n",
      "Epoch 197 Loss: 8.467330535553986\n",
      "Epoch 198 Loss: 170.49921160904708\n",
      "Epoch 199 Loss: 8.541583907263636\n",
      "Epoch 200 Loss: 169.55674943650453\n",
      "Epoch 201 Loss: 8.603435318759063\n",
      "Epoch 202 Loss: 169.77490208102688\n",
      "Epoch 203 Loss: 8.67425332909848\n",
      "Epoch 204 Loss: 168.59408704537145\n",
      "Epoch 205 Loss: 8.733384165061139\n",
      "Epoch 206 Loss: 168.91900803493226\n",
      "Epoch 207 Loss: 8.80414591512964\n",
      "Epoch 208 Loss: 168.2124732066894\n",
      "Epoch 209 Loss: 8.866240716403713\n",
      "Epoch 210 Loss: 168.21693924670203\n",
      "Epoch 211 Loss: 8.93361963535808\n",
      "Epoch 212 Loss: 167.4073707698829\n",
      "Epoch 213 Loss: 8.994058405879969\n",
      "Epoch 214 Loss: 167.6485289477308\n",
      "Epoch 215 Loss: 9.062524906252252\n",
      "Epoch 216 Loss: 166.27370067140666\n",
      "Epoch 217 Loss: 9.11760424400257\n",
      "Epoch 218 Loss: 166.7566524045511\n",
      "Epoch 219 Loss: 9.187044310799232\n",
      "Epoch 220 Loss: 166.19155146403304\n",
      "Epoch 221 Loss: 9.247754427959276\n",
      "Epoch 222 Loss: 165.8599811400987\n",
      "Epoch 223 Loss: 9.309891938968914\n",
      "Epoch 224 Loss: 165.53194248790885\n",
      "Epoch 225 Loss: 9.371631649086748\n",
      "Epoch 226 Loss: 165.20739582735825\n",
      "Epoch 227 Loss: 9.432973119153525\n",
      "Epoch 228 Loss: 164.88630196265493\n",
      "Epoch 229 Loss: 9.493916001876311\n",
      "Epoch 230 Loss: 164.56862217543494\n",
      "Epoch 231 Loss: 9.554460039174712\n",
      "Epoch 232 Loss: 164.2543182185508\n",
      "Epoch 233 Loss: 9.614605059589623\n",
      "Epoch 234 Loss: 163.94335230955474\n",
      "Epoch 235 Loss: 9.67435097575127\n",
      "Epoch 236 Loss: 163.63568712441264\n",
      "Epoch 237 Loss: 9.733697781903663\n",
      "Epoch 238 Loss: 163.33128579136365\n",
      "Epoch 239 Loss: 9.79264555148929\n",
      "Epoch 240 Loss: 163.030111884777\n",
      "Epoch 241 Loss: 9.851194434787299\n",
      "Epoch 242 Loss: 162.73212941917922\n",
      "Epoch 243 Loss: 9.909344656607583\n",
      "Epoch 244 Loss: 162.43730284332923\n",
      "Epoch 245 Loss: 9.967096514037303\n",
      "Epoch 246 Loss: 162.1455970343463\n",
      "Epoch 247 Loss: 10.024450374241939\n",
      "Epoch 248 Loss: 161.85697729200575\n",
      "Epoch 249 Loss: 10.08140667231519\n",
      "Epoch 250 Loss: 161.57148897171146\n",
      "Epoch 251 Loss: 10.13826641598026\n",
      "Epoch 252 Loss: 161.28494686880964\n",
      "Epoch 253 Loss: 10.194117332598795\n",
      "Epoch 254 Loss: 160.89467064838155\n",
      "Epoch 255 Loss: 10.248797872137462\n",
      "Epoch 256 Loss: 160.13254641340293\n",
      "Epoch 257 Loss: 10.298576500615544\n",
      "Epoch 258 Loss: 160.08038078337907\n",
      "Epoch 259 Loss: 10.350155560941213\n",
      "Epoch 260 Loss: 159.56348205031992\n",
      "Epoch 261 Loss: 10.397294029441715\n",
      "Epoch 262 Loss: 159.74808016552564\n",
      "Epoch 263 Loss: 10.450130092818645\n",
      "Epoch 264 Loss: 159.137082402992\n",
      "Epoch 265 Loss: 10.49599001568485\n",
      "Epoch 266 Loss: 159.07409650035396\n",
      "Epoch 267 Loss: 10.545336520475377\n",
      "Epoch 268 Loss: 158.58164144731484\n",
      "Epoch 269 Loss: 10.590912091066773\n",
      "Epoch 270 Loss: 158.6413424868185\n",
      "Epoch 271 Loss: 10.639969768755584\n",
      "Epoch 272 Loss: 158.2706055924997\n",
      "Epoch 273 Loss: 10.685248022334775\n",
      "Epoch 274 Loss: 158.02123890019635\n",
      "Epoch 275 Loss: 10.730179328246829\n",
      "Epoch 276 Loss: 157.88254054932904\n",
      "Epoch 277 Loss: 10.775726997130278\n",
      "Epoch 278 Loss: 157.63724994968567\n",
      "Epoch 279 Loss: 10.819958872799997\n",
      "Epoch 280 Loss: 157.50258295655587\n",
      "Epoch 281 Loss: 10.864810053035205\n",
      "Epoch 282 Loss: 157.26145003862064\n",
      "Epoch 283 Loss: 10.908763068788094\n",
      "Epoch 284 Loss: 157.13500981992834\n",
      "Epoch 285 Loss: 10.952785865286712\n",
      "Epoch 286 Loss: 156.8902235162422\n",
      "Epoch 287 Loss: 10.99525692264495\n",
      "Epoch 288 Loss: 156.64491971498987\n",
      "Epoch 289 Loss: 11.035234301276256\n",
      "Epoch 290 Loss: 156.40214431299916\n",
      "Epoch 291 Loss: 11.074893586489221\n",
      "Epoch 292 Loss: 156.2832378644676\n",
      "Epoch 293 Loss: 11.11531949521753\n",
      "Epoch 294 Loss: 156.1655957338898\n",
      "Epoch 295 Loss: 11.155419601338648\n",
      "Epoch 296 Loss: 155.815030266907\n",
      "Epoch 297 Loss: 11.19311397705018\n",
      "Epoch 298 Loss: 155.70221003452278\n",
      "Epoch 299 Loss: 11.232588099124191\n",
      "Epoch 300 Loss: 155.5905888493578\n",
      "Epoch 301 Loss: 11.271741397433642\n",
      "Epoch 302 Loss: 155.3673471199198\n",
      "Epoch 303 Loss: 11.309577058933979\n",
      "Epoch 304 Loss: 155.14646979834333\n",
      "Epoch 305 Loss: 11.347526330253348\n",
      "Epoch 306 Loss: 155.0409419833022\n",
      "Epoch 307 Loss: 11.385163871440342\n",
      "Epoch 308 Loss: 154.82298552774185\n",
      "Epoch 309 Loss: 11.421485695840238\n",
      "Epoch 310 Loss: 154.72081531189477\n",
      "Epoch 311 Loss: 11.458515030772709\n",
      "Epoch 312 Loss: 154.50618034740722\n",
      "Epoch 313 Loss: 11.494232125782919\n",
      "Epoch 314 Loss: 154.51939423909033\n",
      "Epoch 315 Loss: 11.531664374784617\n",
      "Epoch 316 Loss: 154.08735465495988\n",
      "Epoch 317 Loss: 11.565251188827297\n",
      "Epoch 318 Loss: 154.2237407330868\n",
      "Epoch 319 Loss: 11.602525646547031\n",
      "Epoch 320 Loss: 153.56282909942354\n",
      "Epoch 321 Loss: 11.632390612151205\n",
      "Epoch 322 Loss: 153.8123577602016\n",
      "Epoch 323 Loss: 11.67006745347812\n",
      "Epoch 324 Loss: 153.28315808280126\n",
      "Epoch 325 Loss: 11.70051004807283\n",
      "Epoch 326 Loss: 153.53558436552737\n",
      "Epoch 327 Loss: 11.737614225251278\n",
      "Epoch 328 Loss: 153.00925111332754\n",
      "Epoch 329 Loss: 11.767487597894748\n",
      "Epoch 330 Loss: 153.26451328176987\n",
      "Epoch 331 Loss: 11.804026303662102\n",
      "Epoch 332 Loss: 152.7414622635712\n",
      "Epoch 333 Loss: 11.83460001485729\n",
      "Epoch 334 Loss: 152.67283820637226\n",
      "Epoch 335 Loss: 11.865917107732809\n",
      "Epoch 336 Loss: 152.36573054740302\n",
      "Epoch 337 Loss: 11.8948237540833\n",
      "Epoch 338 Loss: 152.7361218703264\n",
      "Epoch 339 Loss: 11.929508531024187\n",
      "Epoch 340 Loss: 152.31677057083792\n",
      "Epoch 341 Loss: 11.958158759814769\n",
      "Epoch 342 Loss: 152.11046760481082\n",
      "Epoch 343 Loss: 11.985766906305603\n",
      "Epoch 344 Loss: 152.02397865287304\n",
      "Epoch 345 Loss: 12.013134255499907\n",
      "Epoch 346 Loss: 151.9384498458334\n",
      "Epoch 347 Loss: 12.041137228530685\n",
      "Epoch 348 Loss: 151.6148408106798\n",
      "Epoch 349 Loss: 12.064661062033535\n",
      "Epoch 350 Loss: 151.64697816592601\n",
      "Epoch 351 Loss: 12.091153993301944\n",
      "Epoch 352 Loss: 151.45056679388944\n",
      "Epoch 353 Loss: 12.115776391976254\n",
      "Epoch 354 Loss: 151.36898193697894\n",
      "Epoch 355 Loss: 12.140589082926775\n",
      "Epoch 356 Loss: 151.27165300020067\n",
      "Epoch 357 Loss: 12.164061943456144\n",
      "Epoch 358 Loss: 151.17532106619177\n",
      "Epoch 359 Loss: 12.187326548004238\n",
      "Epoch 360 Loss: 151.07997567081813\n",
      "Epoch 361 Loss: 12.210384460078535\n",
      "Epoch 362 Loss: 151.10976335098204\n",
      "Epoch 363 Loss: 12.23479895522616\n",
      "Epoch 364 Loss: 150.89413106308947\n",
      "Epoch 365 Loss: 12.255764933925228\n",
      "Epoch 366 Loss: 150.7889461531133\n",
      "Epoch 367 Loss: 12.277571005085532\n",
      "Epoch 368 Loss: 150.80890700085624\n",
      "Epoch 369 Loss: 12.300354226642636\n",
      "Epoch 370 Loss: 150.5963961999631\n",
      "Epoch 371 Loss: 12.321215869581899\n",
      "Epoch 372 Loss: 150.49882521989662\n",
      "Epoch 373 Loss: 12.341890052196215\n",
      "Epoch 374 Loss: 150.5104738786365\n",
      "Epoch 375 Loss: 12.363404565098822\n",
      "Epoch 376 Loss: 150.4138027771812\n",
      "Epoch 377 Loss: 12.384184017148188\n",
      "Epoch 378 Loss: 150.19848619334283\n",
      "Epoch 379 Loss: 12.403557937765713\n",
      "Epoch 380 Loss: 150.10485911689523\n",
      "Epoch 381 Loss: 12.42167800184298\n",
      "Epoch 382 Loss: 150.0122529621391\n",
      "Epoch 383 Loss: 12.440089689602875\n",
      "Epoch 384 Loss: 150.02739608391266\n",
      "Epoch 385 Loss: 12.458333221965617\n",
      "Epoch 386 Loss: 149.92358772834496\n",
      "Epoch 387 Loss: 12.475285505519812\n",
      "Epoch 388 Loss: 149.8208297931919\n",
      "Epoch 389 Loss: 12.492083473155926\n",
      "Epoch 390 Loss: 149.71911150409738\n",
      "Epoch 391 Loss: 12.508728370722718\n",
      "Epoch 392 Loss: 149.84869855342492\n",
      "Epoch 393 Loss: 12.52739526675396\n",
      "Epoch 394 Loss: 149.62673578025743\n",
      "Epoch 395 Loss: 12.542576470581595\n",
      "Epoch 396 Loss: 149.63847171099468\n",
      "Epoch 397 Loss: 12.56028792500256\n",
      "Epoch 398 Loss: 149.5307509680168\n",
      "Epoch 399 Loss: 12.57564635187796\n",
      "Epoch 400 Loss: 149.42411951987003\n",
      "Epoch 401 Loss: 12.59139958414207\n",
      "Epoch 402 Loss: 149.44401376589758\n",
      "Epoch 403 Loss: 12.60700734402663\n",
      "Epoch 404 Loss: 149.46372111659105\n",
      "Epoch 405 Loss: 12.622470851040474\n",
      "Epoch 406 Loss: 149.48324320693482\n",
      "Epoch 407 Loss: 12.637791317124625\n",
      "Epoch 408 Loss: 149.50258166017952\n",
      "Epoch 409 Loss: 12.652969946636702\n",
      "Epoch 410 Loss: 149.19301289452383\n",
      "Epoch 411 Loss: 12.665394251262645\n",
      "Epoch 412 Loss: 149.3303273811207\n",
      "Epoch 413 Loss: 12.680318261140414\n",
      "Epoch 414 Loss: 149.2434786451171\n",
      "Epoch 415 Loss: 12.694564689718119\n",
      "Epoch 416 Loss: 148.9483564797769\n",
      "Epoch 417 Loss: 12.705629757175544\n",
      "Epoch 418 Loss: 149.08811899314813\n",
      "Epoch 419 Loss: 12.720662656244386\n",
      "Epoch 420 Loss: 149.1187514031557\n",
      "Epoch 421 Loss: 12.734534485251432\n",
      "Epoch 422 Loss: 148.82490083844482\n",
      "Epoch 423 Loss: 12.745228026609173\n",
      "Epoch 424 Loss: 149.07701140837563\n",
      "Epoch 425 Loss: 12.760947028239062\n",
      "Epoch 426 Loss: 148.67904058119174\n",
      "Epoch 427 Loss: 12.770422613578582\n",
      "Epoch 428 Loss: 149.16757411060604\n",
      "Epoch 429 Loss: 12.788129887232829\n",
      "Epoch 430 Loss: 148.76871532438938\n",
      "Epoch 431 Loss: 12.797349573811392\n",
      "Epoch 432 Loss: 148.91367400786305\n",
      "Epoch 433 Loss: 12.811560945716751\n",
      "Epoch 434 Loss: 148.51736504333752\n",
      "Epoch 435 Loss: 12.820563904413078\n",
      "Epoch 436 Loss: 148.89979651469937\n",
      "Epoch 437 Loss: 12.83678089105257\n",
      "Epoch 438 Loss: 148.50364079555013\n",
      "Epoch 439 Loss: 12.845546950377882\n",
      "Epoch 440 Loss: 148.88622452897562\n",
      "Epoch 441 Loss: 12.861529940590616\n",
      "Epoch 442 Loss: 148.4902184170794\n",
      "Epoch 443 Loss: 12.870063212906723\n",
      "Epoch 444 Loss: 148.65829708242578\n",
      "Epoch 445 Loss: 12.88379142827128\n",
      "Epoch 446 Loss: 148.4893786718948\n",
      "Epoch 447 Loss: 12.894243624531011\n",
      "Epoch 448 Loss: 148.65747962414653\n",
      "Epoch 449 Loss: 12.907744411975195\n",
      "Epoch 450 Loss: 148.37497246808135\n",
      "Epoch 451 Loss: 12.917358150784862\n",
      "Epoch 452 Loss: 148.52910494019545\n",
      "Epoch 453 Loss: 12.929929583080265\n",
      "Epoch 454 Loss: 148.24540167429595\n",
      "Epoch 455 Loss: 12.938280458096097\n",
      "Epoch 456 Loss: 148.52442591999286\n",
      "Epoch 457 Loss: 12.951827223740999\n",
      "Epoch 458 Loss: 148.2407807237392\n",
      "Epoch 459 Loss: 12.959970952515715\n",
      "Epoch 460 Loss: 148.51986274732965\n",
      "Epoch 461 Loss: 12.973312922836948\n",
      "Epoch 462 Loss: 148.2362740120717\n",
      "Epoch 463 Loss: 12.981253165974714\n",
      "Epoch 464 Loss: 148.40832223193112\n",
      "Epoch 465 Loss: 12.993391425624143\n",
      "Epoch 466 Loss: 148.23689800426783\n",
      "Epoch 467 Loss: 13.00219564564041\n",
      "Epoch 468 Loss: 148.06718933008426\n",
      "Epoch 469 Loss: 13.010917584562515\n",
      "Epoch 470 Loss: 148.3480291615766\n",
      "Epoch 471 Loss: 13.023777334311351\n",
      "Epoch 472 Loss: 148.07549024783933\n",
      "Epoch 473 Loss: 13.03133650396952\n",
      "Epoch 474 Loss: 148.24916749755224\n",
      "Epoch 475 Loss: 13.043000142648328\n",
      "Epoch 476 Loss: 147.97762442186652\n",
      "Epoch 477 Loss: 13.050377161042269\n",
      "Epoch 478 Loss: 148.25937696367367\n",
      "Epoch 479 Loss: 13.062862409348735\n",
      "Epoch 480 Loss: 147.88015973536818\n",
      "Epoch 481 Loss: 13.069506308656912\n",
      "Epoch 482 Loss: 147.96044946756206\n",
      "Epoch 483 Loss: 13.07934780506669\n",
      "Epoch 484 Loss: 147.8027936851592\n",
      "Epoch 485 Loss: 13.086849544670198\n",
      "Epoch 486 Loss: 147.98832827697956\n",
      "Epoch 487 Loss: 13.097498034942483\n",
      "Epoch 488 Loss: 147.72278379936404\n",
      "Epoch 489 Loss: 13.103812066653516\n",
      "Epoch 490 Loss: 148.01616763254628\n",
      "Epoch 491 Loss: 13.115301064183308\n",
      "Epoch 492 Loss: 147.5157152273714\n",
      "Epoch 493 Loss: 13.119214875837551\n",
      "Epoch 494 Loss: 148.0240756469593\n",
      "Epoch 495 Loss: 13.132564365408308\n",
      "Epoch 496 Loss: 147.52355073831512\n",
      "Epoch 497 Loss: 13.13631229096092\n",
      "Epoch 498 Loss: 147.93015133269625\n",
      "Epoch 499 Loss: 13.148540598178494\n",
      "(array([-1.,  1.]), array([ 693, 4507], dtype=int64))\n",
      "Training Accuracy SVM: 0.56\n"
     ]
    }
   ],
   "source": [
    "weights = run_svm(X_train, y_train, 0.01, 500, 0.5)\n",
    "train_accuracy = accuracy(X_train, y_train, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-1.,  1.]), array([ 166, 1134], dtype=int64))\n",
      "Training Accuracy SVM: 0.55\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('./testSet.csv')\n",
    "X_test, y_test = get_features_labels(test_data)\n",
    "accuracy(X_test, y_test, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q3\n",
    "train_set = pd.read_csv('./trainingSet.csv')\n",
    "#shuffling\n",
    "train_set = train_set.sample(random_state=18, frac=1)\n",
    "fold_number = 10\n",
    "fold_size=len(train_set)/fold_number\n",
    "fold_data_list=[]\n",
    "for i in range(10):\n",
    "    new_fold=train_set.iloc[int(i*fold_size):int((i+1)*fold_size),:]\n",
    "    fold_data_list.append(new_fold)\n",
    "len(fold_data_list[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm(X, y, lamb, max_iter, step_size, add_intercept):\n",
    "    if(add_intercept):\n",
    "        bias = np.ones((X.shape[0], 1))\n",
    "        X = np.concatenate((bias, X), axis=1)\n",
    "    weights = np.zeros(X.shape[1])\n",
    "    for epoch in range(max_iter):\n",
    "        # prev_weights = weights\n",
    "        dw = gradient_calculation(X, y, weights, lamb)\n",
    "        diff =  step_size * dw\n",
    "        weights -= diff\n",
    "        if(norm(diff)< 0.000001):\n",
    "            # print(\"Tolerance reached\")\n",
    "            break;\n",
    "        # print('Epoch {} Loss: {}'.format(epoch, loss_function(X, y, weights, lamb)))\n",
    "    return weights \n",
    "def model_accuracy(X, y, weights):\n",
    "    bias = np.ones((X.shape[0], 1))\n",
    "    X = np.concatenate((bias, X), axis=1)\n",
    "    preds = predict(X, weights)\n",
    "    t = np.where(y <= 0, -1, 1)\n",
    "    accuracy = (preds == t).sum()/t.size\n",
    "    return accuracy\n",
    "\n",
    "def get_accuracy(X_train, y_train, X_test, y_test, weights):\n",
    "    print(\"Training Accuracy SVM:\" , round(model_accuracy(X_train, y_train, weights), 2))\n",
    "    print(\"Testing Accuracy SVM:\" , round(model_accuracy(X_test, y_test, weights), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of frac: 0.025\n",
      "Value of ind:  0\n",
      "Training Accuracy SVM: 0.56\n",
      "Testing Accuracy SVM: 0.59\n",
      "Value of ind:  1\n",
      "Training Accuracy SVM: 0.57\n",
      "Testing Accuracy SVM: 0.65\n",
      "Value of ind:  2\n",
      "Training Accuracy SVM: 0.59\n",
      "Testing Accuracy SVM: 0.53\n",
      "Value of ind:  3\n",
      "Training Accuracy SVM: 0.56\n",
      "Testing Accuracy SVM: 0.61\n",
      "Value of ind:  4\n",
      "Training Accuracy SVM: 0.56\n",
      "Testing Accuracy SVM: 0.55\n",
      "Value of ind:  5\n",
      "Training Accuracy SVM: 0.56\n",
      "Testing Accuracy SVM: 0.57\n",
      "Value of ind:  6\n",
      "Training Accuracy SVM: 0.56\n",
      "Testing Accuracy SVM: 0.53\n",
      "Value of ind:  7\n",
      "Training Accuracy SVM: 0.55\n",
      "Testing Accuracy SVM: 0.49\n",
      "Value of ind:  8\n",
      "Training Accuracy SVM: 0.59\n",
      "Testing Accuracy SVM: 0.52\n",
      "Value of ind:  9\n",
      "Training Accuracy SVM: 0.59\n",
      "Testing Accuracy SVM: 0.53\n",
      "Value of frac: 0.05\n",
      "Value of ind:  0\n",
      "Training Accuracy SVM: 0.53\n",
      "Testing Accuracy SVM: 0.56\n",
      "Value of ind:  1\n",
      "Training Accuracy SVM: 0.54\n",
      "Testing Accuracy SVM: 0.48\n",
      "Value of ind:  2\n",
      "Training Accuracy SVM: 0.54\n",
      "Testing Accuracy SVM: 0.52\n",
      "Value of ind:  3\n",
      "Training Accuracy SVM: 0.57\n",
      "Testing Accuracy SVM: 0.55\n",
      "Value of ind:  4\n",
      "Training Accuracy SVM: 0.56\n",
      "Testing Accuracy SVM: 0.55\n",
      "Value of ind:  5\n",
      "Training Accuracy SVM: 0.59\n",
      "Testing Accuracy SVM: 0.59\n",
      "Value of ind:  6\n",
      "Training Accuracy SVM: 0.58\n",
      "Testing Accuracy SVM: 0.55\n",
      "Value of ind:  7\n",
      "Training Accuracy SVM: 0.57\n",
      "Testing Accuracy SVM: 0.58\n",
      "Value of ind:  8\n",
      "Training Accuracy SVM: 0.6\n",
      "Testing Accuracy SVM: 0.59\n",
      "Value of ind:  9\n",
      "Training Accuracy SVM: 0.59\n",
      "Testing Accuracy SVM: 0.57\n",
      "Value of frac: 0.075\n",
      "Value of ind:  0\n",
      "Training Accuracy SVM: 0.57\n",
      "Testing Accuracy SVM: 0.55\n",
      "Value of ind:  1\n",
      "Training Accuracy SVM: 0.56\n",
      "Testing Accuracy SVM: 0.49\n",
      "Value of ind:  2\n",
      "Training Accuracy SVM: 0.58\n",
      "Testing Accuracy SVM: 0.59\n",
      "Value of ind:  3\n",
      "Training Accuracy SVM: 0.56\n",
      "Testing Accuracy SVM: 0.53\n",
      "Value of ind:  4\n",
      "Training Accuracy SVM: 0.59\n",
      "Testing Accuracy SVM: 0.55\n",
      "Value of ind:  5\n",
      "Training Accuracy SVM: 0.58\n",
      "Testing Accuracy SVM: 0.57\n",
      "Value of ind:  6\n",
      "Training Accuracy SVM: 0.59\n",
      "Testing Accuracy SVM: 0.56\n",
      "Value of ind:  7\n",
      "Training Accuracy SVM: 0.58\n",
      "Testing Accuracy SVM: 0.58\n",
      "Value of ind:  8\n",
      "Training Accuracy SVM: 0.59\n",
      "Testing Accuracy SVM: 0.59\n",
      "Value of ind:  9\n",
      "Training Accuracy SVM: 0.56\n",
      "Testing Accuracy SVM: 0.56\n",
      "Value of frac: 0.1\n",
      "Value of ind:  0\n",
      "Training Accuracy SVM: 0.58\n",
      "Testing Accuracy SVM: 0.54\n",
      "Value of ind:  1\n",
      "Training Accuracy SVM: 0.57\n",
      "Testing Accuracy SVM: 0.52\n",
      "Value of ind:  2\n",
      "Training Accuracy SVM: 0.59\n",
      "Testing Accuracy SVM: 0.58\n",
      "Value of ind:  3\n",
      "Training Accuracy SVM: 0.58\n",
      "Testing Accuracy SVM: 0.57\n",
      "Value of ind:  4\n",
      "Training Accuracy SVM: 0.59\n",
      "Testing Accuracy SVM: 0.55\n",
      "Value of ind:  5\n",
      "Training Accuracy SVM: 0.56\n",
      "Testing Accuracy SVM: 0.58\n",
      "Value of ind:  6\n",
      "Training Accuracy SVM: 0.59\n",
      "Testing Accuracy SVM: 0.56\n",
      "Value of ind:  7\n",
      "Training Accuracy SVM: 0.57\n",
      "Testing Accuracy SVM: 0.55\n",
      "Value of ind:  8\n",
      "Training Accuracy SVM: 0.59\n",
      "Testing Accuracy SVM: 0.59\n",
      "Value of ind:  9\n",
      "Training Accuracy SVM: 0.59\n",
      "Testing Accuracy SVM: 0.57\n",
      "Value of frac: 0.15\n",
      "Value of ind:  0\n",
      "Training Accuracy SVM: 0.53\n",
      "Testing Accuracy SVM: 0.56\n",
      "Value of ind:  1\n",
      "Training Accuracy SVM: 0.6\n",
      "Testing Accuracy SVM: 0.6\n",
      "Value of ind:  2\n",
      "Training Accuracy SVM: 0.6\n",
      "Testing Accuracy SVM: 0.57\n",
      "Value of ind:  3\n",
      "Training Accuracy SVM: 0.61\n",
      "Testing Accuracy SVM: 0.65\n",
      "Value of ind:  4\n",
      "Training Accuracy SVM: 0.56\n",
      "Testing Accuracy SVM: 0.58\n",
      "Value of ind:  5\n",
      "Training Accuracy SVM: 0.58\n",
      "Testing Accuracy SVM: 0.58\n",
      "Value of ind:  6\n",
      "Training Accuracy SVM: 0.59\n",
      "Testing Accuracy SVM: 0.56\n",
      "Value of ind:  7\n",
      "Training Accuracy SVM: 0.57\n",
      "Testing Accuracy SVM: 0.56\n",
      "Value of ind:  8\n",
      "Training Accuracy SVM: 0.56\n",
      "Testing Accuracy SVM: 0.56\n",
      "Value of ind:  9\n",
      "Training Accuracy SVM: 0.56\n",
      "Testing Accuracy SVM: 0.57\n",
      "Value of frac: 0.2\n",
      "Value of ind:  0\n",
      "Training Accuracy SVM: 0.56\n",
      "Testing Accuracy SVM: 0.59\n",
      "Value of ind:  1\n",
      "Training Accuracy SVM: 0.59\n",
      "Testing Accuracy SVM: 0.6\n",
      "Value of ind:  2\n",
      "Training Accuracy SVM: 0.58\n",
      "Testing Accuracy SVM: 0.57\n",
      "Value of ind:  3\n",
      "Training Accuracy SVM: 0.59\n",
      "Testing Accuracy SVM: 0.57\n",
      "Value of ind:  4\n",
      "Training Accuracy SVM: 0.57\n",
      "Testing Accuracy SVM: 0.58\n",
      "Value of ind:  5\n",
      "Training Accuracy SVM: 0.58\n",
      "Testing Accuracy SVM: 0.57\n",
      "Value of ind:  6\n",
      "Training Accuracy SVM: 0.58\n",
      "Testing Accuracy SVM: 0.55\n",
      "Value of ind:  7\n",
      "Training Accuracy SVM: 0.57\n",
      "Testing Accuracy SVM: 0.57\n",
      "Value of ind:  8\n",
      "Training Accuracy SVM: 0.56\n",
      "Testing Accuracy SVM: 0.55\n",
      "Value of ind:  9\n",
      "Training Accuracy SVM: 0.57\n",
      "Testing Accuracy SVM: 0.56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f20c0114c0>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "fraction_list = [0.025,0.05,0.075,0.1,0.15,0.2]\n",
    "svm_data = []\n",
    "lr_data = []\n",
    "f = 0.025\n",
    "ind = 5\n",
    "for f in fraction_list:\n",
    "    svm_record = []\n",
    "    lr_record = []\n",
    "    # print(\"Value of frac:\", f)\n",
    "    for ind in range(10):\n",
    "        test_set = fold_data_list[ind]\n",
    "        rem_set = []\n",
    "        for k, data in enumerate(fold_data_list):\n",
    "            if(k!=ind):\n",
    "                rem_set.append(fold_data_list[k])\n",
    "        new_train_set = pd.concat(rem_set)\n",
    "        new_train_set = new_train_set.sample(random_state=32, frac=f)\n",
    "        new_X_train, new_y_train = get_features_labels(new_train_set)\n",
    "        new_X_test, new_y_test = get_features_labels(test_set)\n",
    "        weights = run_svm(new_X_train, new_y_train, 0.01, 500, 0.5, True)\n",
    "        # print(\"Value of ind: \", ind)\n",
    "        svm_record.append(model_accuracy(new_X_test, new_y_test, weights))\n",
    "        # get_accuracy(new_X_train, new_y_train, new_X_test, new_y_test, weights)\n",
    "    svm_mean = np.mean(svm_record)\n",
    "    svm_std = np.sqrt(np.var(svm_record))\n",
    "    svm_std_err = svm_std/np.sqrt(10)\n",
    "    svm_data.append([f, svm_mean, svm_std_err])\n",
    "svm_data=np.array(svm_data)\n",
    "plt.errorbar(fraction_list, svm_data[:, 1], yerr=svm_data[:, 2], label='SVM')\n",
    "plt.xlabel('t_frac')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "\t    return(1 / (1 + np.exp(-z)))  # z = w.T.dot(x_i)\n",
    "\n",
    "def cost_func(features, target, weights, lamb):\n",
    "    scores = np.dot(features, weights)\n",
    "    cf = np.sum( target*scores - np.log(1 + np.exp(scores)) )  +  lamb/(2) * np.dot(weights, weights)\n",
    "    return cf\n",
    "\n",
    "def logistic_regression(features, target, lamb, num_steps, learning_rate, add_intercept):\n",
    "    if add_intercept:\n",
    "        bias = np.ones((features.shape[0], 1))\n",
    "        features = np.concatenate((bias, features), axis=1)\n",
    "    \n",
    "    weights = np.zeros(features.shape[1]) \n",
    "    \n",
    "    for step in range(0, num_steps):\n",
    "        w1 = np.array(weights)\n",
    "        scores = np.dot(features, weights)\n",
    "        predictions = sigmoid(scores)\n",
    "\n",
    "        # Update weights with gradient\n",
    "       \toutput_error_signal = target - predictions\n",
    "        gradient = np.dot(features.T, output_error_signal) +  (lamb * weights)\n",
    "        weights += learning_rate * gradient\n",
    "        w2 = np.array(weights)\n",
    "        diff = np.subtract(w2, w1)\n",
    "        l2 = norm(diff)\n",
    "        if l2 < 0.000001:\n",
    "        \tbreak\n",
    "        \n",
    "    \t\n",
    "        # Print log-likelihood every so often\n",
    "        #if step % 10 == 0:\n",
    "            #print cost_func(features, target, weights)\n",
    "    return weights\n",
    "\n",
    "def lr_accuracy(X, y, weights):\n",
    "    bias = np.ones((X.shape[0], 1))\n",
    "    X = np.concatenate((bias, X), axis=1)\n",
    "    preds = np.round(sigmoid(np.dot(X, weights)))\n",
    "    accuracy = (preds == y).sum().astype(float) / len(preds)\n",
    "    return round(accuracy, 2)\n",
    "\n",
    "def print_accuracy(train_features, train_labels, X_test, test_label, weights):\n",
    "    print ('Training Accuracy LR:', lr_accuracy(train_features, train_labels, weights))\n",
    "    print ('Testing Accuracy LR:', lr_accuracy(X_test, test_label, weights))\n",
    "\n",
    "# run this if modelIdx = 1\n",
    "def run_lr(X_train, y_train, X_test, y_test):\n",
    "    lamb = 0.01 \n",
    "    weights = logistic_regression(X_train, y_train, lamb=lamb, num_steps = 500, learning_rate = 0.01, add_intercept=True)\n",
    "    print_accuracy(X_train, y_train, X_test, y_test, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train type <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garga\\AppData\\Local\\Temp/ipykernel_20896/1873876140.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return(1 / (1 + np.exp(-z)))  # z = w.T.dot(x_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy LR: 0.66\n",
      "Testing Accuracy LR: 0.66\n"
     ]
    }
   ],
   "source": [
    "    train_data = pd.read_csv('./trainingSet.csv')\n",
    "    X_train, y_train = get_features_labels(train_data)\n",
    "    print(\"x train type\", type(X_train))\n",
    "    test_data = pd.read_csv('./testSet.csv')\n",
    "    X_test, y_test = get_features_labels(test_data)\n",
    "\n",
    "    run_lr(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kfold_split(input_filename, fold_number):\n",
    "    train_set = pd.read_csv(input_filename)\n",
    "    #shuffling\n",
    "    train_set = train_set.sample(random_state=18, frac=1)\n",
    "    fold_number = 10\n",
    "    fold_size=len(train_set)/fold_number\n",
    "    fold_data_list=[]\n",
    "    for i in range(fold_number):\n",
    "        new_fold=train_set.iloc[int(i*fold_size):int((i+1)*fold_size),:]\n",
    "        fold_data_list.append(new_fold)\n",
    "    return fold_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_data_list_nbc = get_kfold_split('./trainingSet_NBC.csv', 10)\n",
    "len(fold_data_list_nbc[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(fold_data_list, fraction, ind):\n",
    "    test_set = fold_data_list[ind]\n",
    "    rem_set = []\n",
    "    for k, data in enumerate(fold_data_list):\n",
    "        if(k!=ind):\n",
    "            rem_set.append(fold_data_list[k])\n",
    "    new_train_set = pd.concat(rem_set)\n",
    "    new_train_set = new_train_set.sample(random_state=32, frac=fraction)\n",
    "    new_X_train, new_y_train = get_features_labels(new_train_set)\n",
    "    new_X_test, new_y_test = get_features_labels(test_set)\n",
    "    return new_X_train, new_y_train, new_X_test, new_y_test\n",
    "# for nbc\n",
    "new_X_train, new_y_train, new_X_test, new_y_test = get_train_test_data(fold_data_list_nbc, 0.025, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[117.0, 234.0, 351.0, 468.0, 702.0, 936.0]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraction_list = [0.025,0.05,0.075,0.1,0.15,0.2]\n",
    "dataset_size = [element*4680 for element in fraction_list]\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
