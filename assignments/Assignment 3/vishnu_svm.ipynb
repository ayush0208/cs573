{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize(data_to_noramlize):\n",
    "    \n",
    "    #Converting Training Data from type Dataframe to Array for Matrix Operation\n",
    "    pure_training_data=np.array(data_to_noramlize)\n",
    "    \n",
    "    #Dividing Each Column of Training Data with respective Column Maximium Value to Normalize the Dataset\n",
    "    return(pure_training_data/np.max(pure_training_data,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self) -> None:\n",
    "        self.lr = 0.5\n",
    "        self.lam = 0.001\n",
    "        self.max_iter = 500\n",
    "        self.tol = 1e-6\n",
    "\n",
    "        self._weigths = None\n",
    "    \n",
    "    def _hinge_loss(self, X, y):\n",
    "        # calculate hinge loss\n",
    "        N = X.shape[0]\n",
    "        distances = 1 - y * (np.dot(X, self._weigths))\n",
    "        distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
    "        hinge_loss =  (np.sum(distances) / N)\n",
    "    \n",
    "        # calculate cost\n",
    "        loss = self.lam / 2 * np.dot(self._weigths, self._weigths) + hinge_loss\n",
    "        return loss\n",
    "\n",
    "    def _calc_gradient(self, X, y):\n",
    "        distance = 1 - y * (np.dot(X, self._weigths))\n",
    "        dw = np.zeros(len(self._weigths))\n",
    "        t = np.where(y <= 0, -1, 1)\n",
    "        for ind, d in enumerate(distance):\n",
    "            condition = t[ind] * (np.dot(X[ind], self._weigths)) >= 1\n",
    "            if condition:\n",
    "                dw += 2 * self.lam * self._weigths\n",
    "            else:\n",
    "                dw += 2 * self.lam * self._weigths - np.dot(X[ind], t[ind])\n",
    "        return dw/len(y)\n",
    "            \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        bias = np.ones((X.shape[0], 1))\n",
    "        X = np.concatenate((bias, X), axis=1)\n",
    "        self._weigths = np.zeros(X.shape[1])\n",
    "        for epoch in range(self.max_iter):\n",
    "            idx = np.random.permutation(len(X))\n",
    "            x_shuffled = X[idx]\n",
    "            y_shuffled = y[idx]\n",
    "\n",
    "            dw = self._calc_gradient(x_shuffled, y_shuffled)\n",
    "            self._weigths -= self.lr * dw\n",
    "\n",
    "            print('Epoch {} Loss: {}'.format(epoch, self._hinge_loss(X, y)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self._weigths))\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        bias = np.ones((X.shape[0], 1))\n",
    "        X = np.concatenate((bias, X), axis=1)\n",
    "        preds = self.predict(X)\n",
    "        t = np.where(y <= 0, -1, 1)\n",
    "        print(np.unique(preds, return_counts=True))\n",
    "        return (preds == t).sum()/t.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_labels(df):\n",
    "    labels = df['decision']\n",
    "    train_features = df.drop('decision', 1)\n",
    "    return train_features.to_numpy(), labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./trainingSet.csv')\n",
    "X_train, y_train = get_features_labels(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 73.88518002490112\n",
      "Epoch 1 Loss: 0.6007227826492731\n",
      "Epoch 2 Loss: 146.90133295442288\n",
      "Epoch 3 Loss: 0.5809779330469369\n",
      "Epoch 4 Loss: 219.78267044977133\n",
      "Epoch 5 Loss: 0.5723941323534867\n",
      "Epoch 6 Loss: 292.5294174983908\n",
      "Epoch 7 Loss: 26.676691714711538\n",
      "Epoch 8 Loss: 0.6284350546813755\n",
      "Epoch 9 Loss: 99.7841269352965\n",
      "Epoch 10 Loss: 0.6055535030450271\n",
      "Epoch 11 Loss: 172.75657657978033\n",
      "Epoch 12 Loss: 0.5938515773302279\n",
      "Epoch 13 Loss: 245.59426592650934\n",
      "Epoch 14 Loss: 0.6213133906222962\n",
      "Epoch 15 Loss: 254.9677306693524\n",
      "Epoch 16 Loss: 1.065480748536354\n",
      "Epoch 17 Loss: 125.79008862609064\n",
      "Epoch 18 Loss: 0.616444071157469\n",
      "Epoch 19 Loss: 198.71701337353957\n",
      "Epoch 20 Loss: 0.6111684799691923\n",
      "Epoch 21 Loss: 271.30331788410905\n",
      "Epoch 22 Loss: 8.26256371947386\n",
      "Epoch 23 Loss: 0.637218803447034\n",
      "Epoch 24 Loss: 183.63238504320725\n",
      "Epoch 25 Loss: 0.6317611960176102\n",
      "Epoch 26 Loss: 256.2487036515593\n",
      "Epoch 27 Loss: 2.4448105032866243\n",
      "Epoch 28 Loss: 32.32162678623083\n",
      "Epoch 29 Loss: 0.6837051647823391\n",
      "Epoch 30 Loss: 111.65162780521567\n",
      "Epoch 31 Loss: 0.6702399688100215\n",
      "Epoch 32 Loss: 184.60986211196297\n",
      "Epoch 33 Loss: 0.6678975252247746\n",
      "Epoch 34 Loss: 255.17128972603555\n",
      "Epoch 35 Loss: 3.229509158711455\n",
      "Epoch 36 Loss: 18.87106612976491\n",
      "Epoch 37 Loss: 0.708061604514624\n",
      "Epoch 38 Loss: 140.33965258865587\n",
      "Epoch 39 Loss: 0.7013103666888733\n",
      "Epoch 40 Loss: 213.13853197556477\n",
      "Epoch 41 Loss: 0.735611259568599\n",
      "Epoch 42 Loss: 240.82259001924498\n",
      "Epoch 43 Loss: 1.5563590346604634\n",
      "Epoch 44 Loss: 115.22332975906609\n",
      "Epoch 45 Loss: 0.7367180207256676\n",
      "Epoch 46 Loss: 188.17953929743095\n",
      "Epoch 47 Loss: 0.7422002183211915\n",
      "Epoch 48 Loss: 245.93827267105357\n",
      "Epoch 49 Loss: 2.7061336196947097\n",
      "Epoch 50 Loss: 55.00341598262159\n",
      "Epoch 51 Loss: 0.7879286032267732\n",
      "Epoch 52 Loss: 132.10571237163853\n",
      "Epoch 53 Loss: 0.7845786751291779\n",
      "Epoch 54 Loss: 204.16457009899568\n",
      "Epoch 55 Loss: 0.8501699745190789\n",
      "Epoch 56 Loss: 219.47227091356348\n",
      "Epoch 57 Loss: 1.0730001666793008\n",
      "Epoch 58 Loss: 173.71132221004265\n",
      "Epoch 59 Loss: 0.8200147061234977\n",
      "Epoch 60 Loss: 230.5045471983195\n",
      "Epoch 61 Loss: 1.7571810788608686\n",
      "Epoch 62 Loss: 116.47839743509888\n",
      "Epoch 63 Loss: 0.850253450179297\n",
      "Epoch 64 Loss: 188.56925113080007\n",
      "Epoch 65 Loss: 0.8976575465810619\n",
      "Epoch 66 Loss: 218.18008037880318\n",
      "Epoch 67 Loss: 1.3265874125363595\n",
      "Epoch 68 Loss: 152.8383081528468\n",
      "Epoch 69 Loss: 0.8872408673835759\n",
      "Epoch 70 Loss: 215.19087994777428\n",
      "Epoch 71 Loss: 1.3458092292670565\n",
      "Epoch 72 Loss: 152.37327336763622\n",
      "Epoch 73 Loss: 0.9184875965226804\n",
      "Epoch 74 Loss: 212.2461943905803\n",
      "Epoch 75 Loss: 1.3675878398517725\n",
      "Epoch 76 Loss: 151.7803234923946\n",
      "Epoch 77 Loss: 0.9505189477463192\n",
      "Epoch 78 Loss: 209.0819138116669\n",
      "Epoch 79 Loss: 1.3832989017193178\n",
      "Epoch 80 Loss: 152.0860260122042\n",
      "Epoch 81 Loss: 0.983650772725176\n",
      "Epoch 82 Loss: 205.75969987175822\n",
      "Epoch 83 Loss: 1.3954800053212946\n",
      "Epoch 84 Loss: 151.82821584465324\n",
      "Epoch 85 Loss: 1.0172415071354282\n",
      "Epoch 86 Loss: 202.23859891715674\n",
      "Epoch 87 Loss: 1.404320538270465\n",
      "Epoch 88 Loss: 152.92115024878183\n",
      "Epoch 89 Loss: 1.0540177397162362\n",
      "Epoch 90 Loss: 199.07665957985452\n",
      "Epoch 91 Loss: 1.4248192355610731\n",
      "Epoch 92 Loss: 153.13299434099451\n",
      "Epoch 93 Loss: 1.0941352691693476\n",
      "Epoch 94 Loss: 196.07781063185095\n",
      "Epoch 95 Loss: 1.4501166343783891\n",
      "Epoch 96 Loss: 152.7802430468973\n",
      "Epoch 97 Loss: 1.1356444970107622\n",
      "Epoch 98 Loss: 192.2659594664401\n",
      "Epoch 99 Loss: 1.4581006796701463\n",
      "Epoch 100 Loss: 155.23444275221684\n",
      "Epoch 101 Loss: 1.1862293508768444\n",
      "Epoch 102 Loss: 187.93927780250976\n",
      "Epoch 103 Loss: 1.4596870471135661\n",
      "Epoch 104 Loss: 157.71040720569113\n",
      "Epoch 105 Loss: 1.2400448555041077\n",
      "Epoch 106 Loss: 183.1057207257786\n",
      "Epoch 107 Loss: 1.4572836025565747\n",
      "Epoch 108 Loss: 159.43249259749635\n",
      "Epoch 109 Loss: 1.292418406208078\n",
      "Epoch 110 Loss: 178.0030692849493\n",
      "Epoch 111 Loss: 1.459989664706877\n",
      "Epoch 112 Loss: 162.73063698103988\n",
      "Epoch 113 Loss: 1.3570282130816653\n",
      "Epoch 114 Loss: 171.39226567578675\n",
      "Epoch 115 Loss: 1.4514617682798079\n",
      "Epoch 116 Loss: 164.92311033563752\n",
      "Epoch 117 Loss: 1.426556656930535\n",
      "Epoch 118 Loss: 167.04542184189958\n",
      "Epoch 119 Loss: 1.4748098835173746\n",
      "Epoch 120 Loss: 163.1012715270786\n",
      "Epoch 121 Loss: 1.471858349987441\n",
      "Epoch 122 Loss: 164.27101001841484\n",
      "Epoch 123 Loss: 1.5129226887046485\n",
      "Epoch 124 Loss: 160.66459636814403\n",
      "Epoch 125 Loss: 1.5132257502037518\n",
      "Epoch 126 Loss: 161.619645309067\n",
      "Epoch 127 Loss: 1.5527988466429323\n",
      "Epoch 128 Loss: 158.3818958927263\n",
      "Epoch 129 Loss: 1.5565562935320314\n",
      "Epoch 130 Loss: 158.9887070486583\n",
      "Epoch 131 Loss: 1.5935312792702931\n",
      "Epoch 132 Loss: 156.30928174417625\n",
      "Epoch 133 Loss: 1.6022375698011704\n",
      "Epoch 134 Loss: 156.34888276925486\n",
      "Epoch 135 Loss: 1.6348517720231621\n",
      "Epoch 136 Loss: 154.03513299785308\n",
      "Epoch 137 Loss: 1.6469900274533162\n",
      "Epoch 138 Loss: 153.42264747335315\n",
      "Epoch 139 Loss: 1.674241010867071\n",
      "Epoch 140 Loss: 151.76239093188656\n",
      "Epoch 141 Loss: 1.6987626216715501\n",
      "Epoch 142 Loss: 150.25427586498895\n",
      "Epoch 143 Loss: 1.7205115924872447\n",
      "Epoch 144 Loss: 149.20405815451906\n",
      "Epoch 145 Loss: 1.7453321635666628\n",
      "Epoch 146 Loss: 148.15762159061848\n",
      "Epoch 147 Loss: 1.7691068969030606\n",
      "Epoch 148 Loss: 146.77854714388562\n",
      "Epoch 149 Loss: 1.791861619078524\n",
      "Epoch 150 Loss: 145.64383728548026\n",
      "Epoch 151 Loss: 1.8146919799036754\n",
      "Epoch 152 Loss: 144.50849251822208\n",
      "Epoch 153 Loss: 1.8375892794390236\n",
      "Epoch 154 Loss: 143.49586436086128\n",
      "Epoch 155 Loss: 1.8606373332573702\n",
      "Epoch 156 Loss: 142.3784180823913\n",
      "Epoch 157 Loss: 1.8866264153682106\n",
      "Epoch 158 Loss: 141.00775399664764\n",
      "Epoch 159 Loss: 1.9128124894442102\n",
      "Epoch 160 Loss: 139.64689758733726\n",
      "Epoch 161 Loss: 1.9357669647574947\n",
      "Epoch 162 Loss: 138.64672746503462\n",
      "Epoch 163 Loss: 1.9607106585761733\n",
      "Epoch 164 Loss: 137.53449341107623\n",
      "Epoch 165 Loss: 1.9836697422466938\n",
      "Epoch 166 Loss: 136.43705297116858\n",
      "Epoch 167 Loss: 2.0073493510250096\n",
      "Epoch 168 Loss: 135.6678325273911\n",
      "Epoch 169 Loss: 2.033936412271414\n",
      "Epoch 170 Loss: 134.7439167315407\n",
      "Epoch 171 Loss: 2.0597773114091025\n",
      "Epoch 172 Loss: 133.695109926752\n",
      "Epoch 173 Loss: 2.089817701631903\n",
      "Epoch 174 Loss: 132.43272642053998\n",
      "Epoch 175 Loss: 2.1105998929703533\n",
      "Epoch 176 Loss: 131.50322609822166\n",
      "Epoch 177 Loss: 2.132609938759081\n",
      "Epoch 178 Loss: 130.69922994832578\n",
      "Epoch 179 Loss: 2.160113099652352\n",
      "Epoch 180 Loss: 129.93446198037284\n",
      "Epoch 181 Loss: 2.185409092849607\n",
      "Epoch 182 Loss: 128.71374196420504\n",
      "Epoch 183 Loss: 2.20658728716069\n",
      "Epoch 184 Loss: 127.91633811590937\n",
      "Epoch 185 Loss: 2.231428585375038\n",
      "Epoch 186 Loss: 127.10891422728524\n",
      "Epoch 187 Loss: 2.2569842735492625\n",
      "Epoch 188 Loss: 126.09891050450042\n",
      "Epoch 189 Loss: 2.2799819374212134\n",
      "Epoch 190 Loss: 125.23835664984237\n",
      "Epoch 191 Loss: 2.300841354604885\n",
      "Epoch 192 Loss: 124.49278528355939\n",
      "Epoch 193 Loss: 2.3280513266471785\n",
      "Epoch 194 Loss: 123.48083078082188\n",
      "Epoch 195 Loss: 2.351350261631275\n",
      "Epoch 196 Loss: 122.71036842958905\n",
      "Epoch 197 Loss: 2.373615049458135\n",
      "Epoch 198 Loss: 121.61143350355383\n",
      "Epoch 199 Loss: 2.396962057272111\n",
      "Epoch 200 Loss: 120.2283964611527\n",
      "Epoch 201 Loss: 2.4124546520085914\n",
      "Epoch 202 Loss: 120.26377894705222\n",
      "Epoch 203 Loss: 2.442624581998339\n",
      "Epoch 204 Loss: 118.37312927470228\n",
      "Epoch 205 Loss: 2.4602468567853286\n",
      "Epoch 206 Loss: 117.78938253812021\n",
      "Epoch 207 Loss: 2.486816478810529\n",
      "Epoch 208 Loss: 116.24617488042799\n",
      "Epoch 209 Loss: 2.506254721107059\n",
      "Epoch 210 Loss: 115.69729394820344\n",
      "Epoch 211 Loss: 2.5286074219689527\n",
      "Epoch 212 Loss: 115.05517692717858\n",
      "Epoch 213 Loss: 2.551995271746139\n",
      "Epoch 214 Loss: 113.66841810585458\n",
      "Epoch 215 Loss: 2.5655694950494987\n",
      "Epoch 216 Loss: 113.8076663139143\n",
      "Epoch 217 Loss: 2.5945385962376\n",
      "Epoch 218 Loss: 112.34282515493099\n",
      "Epoch 219 Loss: 2.6115457898079137\n",
      "Epoch 220 Loss: 111.83463477659568\n",
      "Epoch 221 Loss: 2.632754188817929\n",
      "Epoch 222 Loss: 110.91430447190253\n",
      "Epoch 223 Loss: 2.658171640344516\n",
      "Epoch 224 Loss: 110.14252491513635\n",
      "Epoch 225 Loss: 2.6761297784974274\n",
      "Epoch 226 Loss: 109.69506751700384\n",
      "Epoch 227 Loss: 2.6954388034539765\n",
      "Epoch 228 Loss: 109.12303344466983\n",
      "Epoch 229 Loss: 2.712823678745872\n",
      "Epoch 230 Loss: 108.6426281517536\n",
      "Epoch 231 Loss: 2.7398937685000386\n",
      "Epoch 232 Loss: 108.16344784197518\n",
      "Epoch 233 Loss: 2.7627280970960593\n",
      "Epoch 234 Loss: 107.67733843564646\n",
      "Epoch 235 Loss: 2.781941279279186\n",
      "Epoch 236 Loss: 107.09181833896777\n",
      "Epoch 237 Loss: 2.8024408960674805\n",
      "Epoch 238 Loss: 106.16145046749189\n",
      "Epoch 239 Loss: 2.82219422331593\n",
      "Epoch 240 Loss: 105.6576656481734\n",
      "Epoch 241 Loss: 2.8429999078895936\n",
      "Epoch 242 Loss: 104.69135387926492\n",
      "Epoch 243 Loss: 2.858431295209455\n",
      "Epoch 244 Loss: 104.0503159906457\n",
      "Epoch 245 Loss: 2.8827841820805142\n",
      "Epoch 246 Loss: 103.35513879708235\n",
      "Epoch 247 Loss: 2.899332112869782\n",
      "Epoch 248 Loss: 102.98066166732764\n",
      "Epoch 249 Loss: 2.922180749591301\n",
      "Epoch 250 Loss: 102.44460369789434\n",
      "Epoch 251 Loss: 2.946772161847492\n",
      "Epoch 252 Loss: 101.93659999616344\n",
      "Epoch 253 Loss: 2.9654831676320708\n",
      "Epoch 254 Loss: 101.20684640191166\n",
      "Epoch 255 Loss: 2.985551809519415\n",
      "Epoch 256 Loss: 100.7136748081034\n",
      "Epoch 257 Loss: 2.997974631971493\n",
      "Epoch 258 Loss: 100.22099875343756\n",
      "Epoch 259 Loss: 3.0215075989384887\n",
      "Epoch 260 Loss: 99.33895345081032\n",
      "Epoch 261 Loss: 3.0411678459382916\n",
      "Epoch 262 Loss: 98.60550306257696\n",
      "Epoch 263 Loss: 3.052908571907219\n",
      "Epoch 264 Loss: 98.49578273659661\n",
      "Epoch 265 Loss: 3.0728951163375866\n",
      "Epoch 266 Loss: 97.27098700748608\n",
      "Epoch 267 Loss: 3.0767598524351314\n",
      "Epoch 268 Loss: 97.9145045164353\n",
      "Epoch 269 Loss: 3.1064342296500778\n",
      "Epoch 270 Loss: 96.510101680554\n",
      "Epoch 271 Loss: 3.1188028030776085\n",
      "Epoch 272 Loss: 96.09638375077449\n",
      "Epoch 273 Loss: 3.1364977728376866\n",
      "Epoch 274 Loss: 95.66057102365855\n",
      "Epoch 275 Loss: 3.149514311179773\n",
      "Epoch 276 Loss: 95.29983055169187\n",
      "Epoch 277 Loss: 3.167884976409235\n",
      "Epoch 278 Loss: 94.73546507437926\n",
      "Epoch 279 Loss: 3.1801520056689254\n",
      "Epoch 280 Loss: 94.27965149146165\n",
      "Epoch 281 Loss: 3.197709628046104\n",
      "Epoch 282 Loss: 93.70984949437198\n",
      "Epoch 283 Loss: 3.2105016927203796\n",
      "Epoch 284 Loss: 93.5436137858131\n",
      "Epoch 285 Loss: 3.2272513599934745\n",
      "Epoch 286 Loss: 93.45455781358112\n",
      "Epoch 287 Loss: 3.2415233118007025\n",
      "Epoch 288 Loss: 92.98421999245555\n",
      "Epoch 289 Loss: 3.255904462336636\n",
      "Epoch 290 Loss: 92.60800410505463\n",
      "Epoch 291 Loss: 3.2674733276527386\n",
      "Epoch 292 Loss: 92.26417427176872\n",
      "Epoch 293 Loss: 3.283678098341748\n",
      "Epoch 294 Loss: 91.58630669166095\n",
      "Epoch 295 Loss: 3.2886255984526267\n",
      "Epoch 296 Loss: 91.66977409976926\n",
      "Epoch 297 Loss: 3.3028244344552506\n",
      "Epoch 298 Loss: 91.22563667677368\n",
      "Epoch 299 Loss: 3.313725149382181\n",
      "Epoch 300 Loss: 90.96507732439463\n",
      "Epoch 301 Loss: 3.3248471673889046\n",
      "Epoch 302 Loss: 90.7327205144895\n",
      "Epoch 303 Loss: 3.3318394224069836\n",
      "Epoch 304 Loss: 90.59724104754011\n",
      "Epoch 305 Loss: 3.3476865300191894\n",
      "Epoch 306 Loss: 90.40617418413015\n",
      "Epoch 307 Loss: 3.3597552563836692\n",
      "Epoch 308 Loss: 90.3258685846383\n",
      "Epoch 309 Loss: 3.36958724770572\n",
      "Epoch 310 Loss: 90.15665805173184\n",
      "Epoch 311 Loss: 3.381660398276283\n",
      "Epoch 312 Loss: 89.89134238303748\n",
      "Epoch 313 Loss: 3.399910508633454\n",
      "Epoch 314 Loss: 89.48890786718007\n",
      "Epoch 315 Loss: 3.4155095074251323\n",
      "Epoch 316 Loss: 88.74651215218697\n",
      "Epoch 317 Loss: 3.4194048126424246\n",
      "Epoch 318 Loss: 88.77942656923256\n",
      "Epoch 319 Loss: 3.430091870046698\n",
      "Epoch 320 Loss: 88.5185857296508\n",
      "Epoch 321 Loss: 3.4385449884636974\n",
      "Epoch 322 Loss: 88.26249538290672\n",
      "Epoch 323 Loss: 3.4446641460692593\n",
      "Epoch 324 Loss: 88.1990999437862\n",
      "Epoch 325 Loss: 3.4529373473963654\n",
      "Epoch 326 Loss: 87.94400967137582\n",
      "Epoch 327 Loss: 3.4657944166810557\n",
      "Epoch 328 Loss: 87.81069418539836\n",
      "Epoch 329 Loss: 3.4764570574543514\n",
      "Epoch 330 Loss: 87.67315595980004\n",
      "Epoch 331 Loss: 3.4846518007029785\n",
      "Epoch 332 Loss: 87.53626087990769\n",
      "Epoch 333 Loss: 3.4927859605719505\n",
      "Epoch 334 Loss: 87.49580126657588\n",
      "Epoch 335 Loss: 3.505414510092118\n",
      "Epoch 336 Loss: 87.33796328877334\n",
      "Epoch 337 Loss: 3.5218150621084856\n",
      "Epoch 338 Loss: 87.06783720635273\n",
      "Epoch 339 Loss: 3.524150727871251\n",
      "Epoch 340 Loss: 87.01149614144158\n",
      "Epoch 341 Loss: 3.5404430072558455\n",
      "Epoch 342 Loss: 86.97946764129536\n",
      "Epoch 343 Loss: 3.552071614920332\n",
      "Epoch 344 Loss: 86.94614957435648\n",
      "Epoch 345 Loss: 3.565319914670665\n",
      "Epoch 346 Loss: 86.59234911576662\n",
      "Epoch 347 Loss: 3.5715872795548087\n",
      "Epoch 348 Loss: 86.54278146853225\n",
      "Epoch 349 Loss: 3.5799350926964815\n",
      "Epoch 350 Loss: 86.29202250843936\n",
      "Epoch 351 Loss: 3.588125109307682\n",
      "Epoch 352 Loss: 86.33801824377868\n",
      "Epoch 353 Loss: 3.5963114555409357\n",
      "Epoch 354 Loss: 86.28640024774569\n",
      "Epoch 355 Loss: 3.6022243893487853\n",
      "Epoch 356 Loss: 86.23514243611346\n",
      "Epoch 357 Loss: 3.6187651751486793\n",
      "Epoch 358 Loss: 86.26836901134216\n",
      "Epoch 359 Loss: 3.6262855631050934\n",
      "Epoch 360 Loss: 86.20609091560141\n",
      "Epoch 361 Loss: 3.6338724576773336\n",
      "Epoch 362 Loss: 86.05135469822406\n",
      "Epoch 363 Loss: 3.636922128640732\n",
      "Epoch 364 Loss: 86.00261972153133\n",
      "Epoch 365 Loss: 3.6424456511884853\n",
      "Epoch 366 Loss: 85.95401583721761\n",
      "Epoch 367 Loss: 3.6502184630459675\n",
      "Epoch 368 Loss: 85.89929267130726\n",
      "Epoch 369 Loss: 3.6555189924539473\n",
      "Epoch 370 Loss: 85.8447128005217\n",
      "Epoch 371 Loss: 3.663105942816779\n",
      "Epoch 372 Loss: 85.6150794366452\n",
      "Epoch 373 Loss: 3.6642452989792904\n",
      "Epoch 374 Loss: 85.7679483321158\n",
      "Epoch 375 Loss: 3.674305719245873\n",
      "Epoch 376 Loss: 85.5387597884558\n",
      "Epoch 377 Loss: 3.687386492916691\n",
      "Epoch 378 Loss: 85.32781431828248\n",
      "Epoch 379 Loss: 3.6931285465265575\n",
      "Epoch 380 Loss: 85.32313609696487\n",
      "Epoch 381 Loss: 3.698869638593697\n",
      "Epoch 382 Loss: 85.31851151311201\n",
      "Epoch 383 Loss: 3.7072877296473923\n",
      "Epoch 384 Loss: 85.03752344338346\n",
      "Epoch 385 Loss: 3.7175514279768853\n",
      "Epoch 386 Loss: 85.03948685067809\n",
      "Epoch 387 Loss: 3.722999149422935\n",
      "Epoch 388 Loss: 84.9506100671972\n",
      "Epoch 389 Loss: 3.7262306660437687\n",
      "Epoch 390 Loss: 84.76503644993613\n",
      "Epoch 391 Loss: 3.7363794084589723\n",
      "Epoch 392 Loss: 84.7734750487681\n",
      "Epoch 393 Loss: 3.7416943353739347\n",
      "Epoch 394 Loss: 84.6786564302741\n",
      "Epoch 395 Loss: 3.7514934156803807\n",
      "Epoch 396 Loss: 84.77684201761711\n",
      "Epoch 397 Loss: 3.75173696369362\n",
      "Epoch 398 Loss: 84.5800158342338\n",
      "Epoch 399 Loss: 3.75902559557155\n",
      "Epoch 400 Loss: 84.67836092148121\n",
      "Epoch 401 Loss: 3.7663260391993383\n",
      "Epoch 402 Loss: 84.67102660199356\n",
      "Epoch 403 Loss: 3.7710162407345305\n",
      "Epoch 404 Loss: 84.6637161637677\n",
      "Epoch 405 Loss: 3.7757062680734177\n",
      "Epoch 406 Loss: 84.65642952223298\n",
      "Epoch 407 Loss: 3.780396084813657\n",
      "Epoch 408 Loss: 84.6491665931467\n",
      "Epoch 409 Loss: 3.7850856547714766\n",
      "Epoch 410 Loss: 84.54791994191534\n",
      "Epoch 411 Loss: 3.7874113916416805\n",
      "Epoch 412 Loss: 84.54091643525109\n",
      "Epoch 413 Loss: 3.794629645222908\n",
      "Epoch 414 Loss: 84.54308686916119\n",
      "Epoch 415 Loss: 3.7994788635383636\n",
      "Epoch 416 Loss: 84.44190882916581\n",
      "Epoch 417 Loss: 3.8045047920962576\n",
      "Epoch 418 Loss: 84.1740424490864\n",
      "Epoch 419 Loss: 3.802422556476416\n",
      "Epoch 420 Loss: 84.48554768539024\n",
      "Epoch 421 Loss: 3.8122086005779514\n",
      "Epoch 422 Loss: 84.19491150516606\n",
      "Epoch 423 Loss: 3.8124454073059084\n",
      "Epoch 424 Loss: 84.21398407304271\n",
      "Epoch 425 Loss: 3.8176729527875173\n",
      "Epoch 426 Loss: 84.22040636164435\n",
      "Epoch 427 Loss: 3.8226090752110116\n",
      "Epoch 428 Loss: 84.12457059978156\n",
      "Epoch 429 Loss: 3.8270136615231607\n",
      "Epoch 430 Loss: 84.02361145167845\n",
      "Epoch 431 Loss: 3.8315113566487993\n",
      "Epoch 432 Loss: 84.0187765552987\n",
      "Epoch 433 Loss: 3.8360190325709276\n",
      "Epoch 434 Loss: 83.82163695720374\n",
      "Epoch 435 Loss: 3.8402681299117196\n",
      "Epoch 436 Loss: 83.811335867125\n",
      "Epoch 437 Loss: 3.844516647481483\n",
      "Epoch 438 Loss: 83.80106562574586\n",
      "Epoch 439 Loss: 3.8464347842710653\n",
      "Epoch 440 Loss: 83.79817192667166\n",
      "Epoch 441 Loss: 3.850951049282269\n",
      "Epoch 442 Loss: 83.70506628446499\n",
      "Epoch 443 Loss: 3.860460656570453\n",
      "Epoch 444 Loss: 83.42152454222438\n",
      "Epoch 445 Loss: 3.8669540591114533\n",
      "Epoch 446 Loss: 83.51832636453612\n",
      "Epoch 447 Loss: 3.8711737639716857\n",
      "Epoch 448 Loss: 83.5220190599026\n",
      "Epoch 449 Loss: 3.875392556473844\n",
      "Epoch 450 Loss: 83.4357034349041\n",
      "Epoch 451 Loss: 3.8773543417441787\n",
      "Epoch 452 Loss: 83.45176876803826\n",
      "Epoch 453 Loss: 3.8819137007183535\n",
      "Epoch 454 Loss: 83.36861716059352\n",
      "Epoch 455 Loss: 3.8862141449215106\n",
      "Epoch 456 Loss: 83.1935135627689\n",
      "Epoch 457 Loss: 3.885925542291096\n",
      "Epoch 458 Loss: 83.5114355380388\n",
      "Epoch 459 Loss: 3.8956056684846434\n",
      "Epoch 460 Loss: 83.2376271906523\n",
      "Epoch 461 Loss: 3.895321250545739\n",
      "Epoch 462 Loss: 83.27237311475412\n",
      "Epoch 463 Loss: 3.9003031096374823\n",
      "Epoch 464 Loss: 83.30705659067722\n",
      "Epoch 465 Loss: 3.9052820049080568\n",
      "Epoch 466 Loss: 83.14640013264506\n",
      "Epoch 467 Loss: 3.9101536214315398\n",
      "Epoch 468 Loss: 83.09526968560863\n",
      "Epoch 469 Loss: 3.9128365799179945\n",
      "Epoch 470 Loss: 83.0442482270863\n",
      "Epoch 471 Loss: 3.9155209403697997\n",
      "Epoch 472 Loss: 82.99333551212288\n",
      "Epoch 473 Loss: 3.9182066729649394\n",
      "Epoch 474 Loss: 83.04894382262256\n",
      "Epoch 475 Loss: 3.9235252769702864\n",
      "Epoch 476 Loss: 82.90322998309657\n",
      "Epoch 477 Loss: 3.9238718670229376\n",
      "Epoch 478 Loss: 83.05807561236772\n",
      "Epoch 479 Loss: 3.931655658319225\n",
      "Epoch 480 Loss: 82.8221482574648\n",
      "Epoch 481 Loss: 3.934175888248417\n",
      "Epoch 482 Loss: 82.77047027773368\n",
      "Epoch 483 Loss: 3.9366975163220768\n",
      "Epoch 484 Loss: 82.71897762330065\n",
      "Epoch 485 Loss: 3.9441948652826797\n",
      "Epoch 486 Loss: 82.6625111258421\n",
      "Epoch 487 Loss: 3.9486689430311213\n",
      "Epoch 488 Loss: 82.50950834574681\n",
      "Epoch 489 Loss: 3.948418131397514\n",
      "Epoch 490 Loss: 82.561603610765\n",
      "Epoch 491 Loss: 3.9532543492302423\n",
      "Epoch 492 Loss: 82.40887729571186\n",
      "Epoch 493 Loss: 3.9570938155798268\n",
      "Epoch 494 Loss: 82.42638441823988\n",
      "Epoch 495 Loss: 3.9609316737267393\n",
      "Epoch 496 Loss: 82.27297081693449\n",
      "Epoch 497 Loss: 3.9605324524604657\n",
      "Epoch 498 Loss: 82.52218888494443\n",
      "Epoch 499 Loss: 3.96792049515304\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-1.,  1.]), array([1095, 4105], dtype=int64))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6273076923076923"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./testSet.csv')\n",
    "X_test, y_test = get_features_labels(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-1.,  1.]), array([ 265, 1035], dtype=int64))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6230769230769231"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47a62bc900abaa30fdaba9a8a8bbebde41861c65136e600fc26b837ae5d20c8c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
